{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
<<<<<<< Updated upstream
=======
    "import math\n",
    "from collections import defaultdict\n",
>>>>>>> Stashed changes
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41)) #discriminator takes 41 values from our dataset\n",
    "    #model.add(LeakyReLU(alpha=alpha))\n",
    "    #model.add(Dropout(0.3))\n",
    "    #model.add(Dense(layer2))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid')) #outputs 0 to 1, 1 being real and 0 being fake\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=alpha))\n",
    "    #model.add(Dense(layer2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(41, activation='relu'))\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_model(layer1, layer2, layer3, alpha):\n",
    "    optimizer = Adam(0.001)\n",
    "    \n",
    "    #build generator and discriminator (mirrored)\n",
    "    generator = build_generator(layer1, layer2, layer3, alpha)\n",
    "    \n",
    "    discriminator = build_discriminator(layer3, layer2, layer1, alpha)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    #build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return combined, discriminator, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \n",
    "            \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \n",
    "            \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \n",
    "            \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \n",
    "            \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \n",
    "            \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \n",
    "            \"dst_host_srv_rerror_rate\", \"attack_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(combined, discriminator, generator, epochs):\n",
    "    epochs = epochs+1\n",
    "    batch_size = 4999\n",
    "    dataframe = pd.read_csv('../../../../CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    #dataframe = pd.read_csv('CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    \n",
    "    #apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(ll)\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    f = open(\"GANNeptune.txt\", \"a\")\n",
    "    np.savetxt(\"GANNeptune.txt\", dataset, fmt=\"%d\")\n",
    "    f.close()\n",
    "    \n",
    "#    d = defaultdict(LabelEncoder)\n",
    "#     fit = dataframe.apply(lambda x: d[x.name].fit_transform(x))  # fit is encoded dataframe\n",
    "#     dataset = fit.values   # transform to ndarray\n",
    "    #print(fit)\n",
    "    \n",
    "#     print(\"===============================================\")\n",
    "#     print(\"decoded:\")\n",
    "#     print(\"===============================================\")\n",
    "#     decode_test = dataframe_encoded  # take a slice from the ndarray that we want to decode\n",
    "#     #decode_test_df = pd.DataFrame(decode_test, columns=ll)  # turn that ndarray into a dataframe with correct column names and order\n",
    "#     decoded = decode_test.apply(le.inverse_transform)  # decode that dataframe\n",
    "#     print(decoded)\n",
    "    \n",
    "    #labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    #Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(int)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0\n",
    "    prev_g_loss = 0\n",
    "    \n",
    "    #generating a np array of numbers 0..batch_size-1\n",
    "    idx = np.arange(batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #selecting batch_size random attacks from our training data\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        #generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        #create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        #loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % \n",
    "                  (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "            #saving results to txt to track them as the gan is training\n",
    "            f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%d\")\n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "            results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "            y_pred = estimator.predict(results)\n",
    "        \n",
    "            right = (y_pred == 1).sum()\n",
    "            wrong = len(y_pred)-(y_pred == 1).sum()\n",
    "            accuracy = (right/float(right+wrong))\n",
    "            print(\"Number of right predictions: %d\" % right)\n",
    "            print(\"Number of wrong predictions: %d\" % wrong)\n",
    "            print(\"Accuracy: %.4f \" % accuracy)      "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
=======
   "execution_count": 7,
   "metadata": {},
>>>>>>> Stashed changes
   "outputs": [],
   "source": [
    "#Initialize Random Number Generator\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#load dataset\n",
    "dataframe = pd.read_csv(\"../../../../CSV/normalAndNeptune.csv\")\n",
    "#dataframe = pd.read_csv(\"CSV/normalAndNeptune.csv\")\n",
    "\n",
    "#samples n random data points\n",
    "dataframe = dataframe.sample(n=1000000)\n",
    "#LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "#apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(int)\n",
    "Y = dataset[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get validation data\n",
    "validationToTrainRatio = 0.05\n",
    "validationSize = int(validationToTrainRatio * len(X))\n",
    "validationData = X[:validationSize]\n",
    "validationLabels = Y[:validationSize]\n",
    "X = X[validationSize:]\n",
    "Y = Y[validationSize:]\n",
    "\n",
    "#Get test data\n",
    "testToTrainRatio = 0.05\n",
    "testSize = int(testToTrainRatio * len(X))\n",
    "testData = X[:testSize]\n",
    "testLabels = Y[:testSize]\n",
    "X = X[testSize:]\n",
    "Y = Y[testSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 902500 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
<<<<<<< Updated upstream
      " - 1s - loss: 0.5929 - acc: 0.9302 - val_loss: 0.0041 - val_acc: 0.9999\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.0476 - acc: 0.9859 - val_loss: 0.0059 - val_acc: 0.9998\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.0329 - acc: 0.9898 - val_loss: 0.0025 - val_acc: 0.9999\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.0277 - acc: 0.9910 - val_loss: 0.0016 - val_acc: 0.9999\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.0247 - acc: 0.9917 - val_loss: 0.0012 - val_acc: 1.0000\n"
=======
      " - 2s - loss: 0.5897 - acc: 0.9305 - val_loss: 0.0040 - val_acc: 0.9999\n",
      "Epoch 2/5\n",
      " - 1s - loss: 0.0467 - acc: 0.9861 - val_loss: 0.0054 - val_acc: 0.9998\n",
      "Epoch 3/5\n",
      " - 1s - loss: 0.0326 - acc: 0.9898 - val_loss: 0.0031 - val_acc: 0.9999\n",
      "Epoch 4/5\n",
      " - 1s - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0019 - val_acc: 0.9999\n",
      "Epoch 5/5\n",
      " - 1s - loss: 0.0247 - acc: 0.9917 - val_loss: 0.0014 - val_acc: 0.9999\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "estimator = baseline_model(layers=2, units=32, dropout_rate=0.5, input_shape=X.shape[1:], num_classes=1)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = estimator.fit(X,\n",
    "                    Y,\n",
    "                    epochs=5,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(validationData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47500/47500 [==============================] - 0s 5us/step\n",
<<<<<<< Updated upstream
      "[0.0016491864967355995, 0.9998947368421053]\n"
=======
      "[0.001846434220392257, 0.9998736842105264]\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(estimator.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating GAN model\n",
    "combined, discriminator, generator = GAN_model(8, 16, 32, 0.2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
=======
   "execution_count": 17,
>>>>>>> Stashed changes
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "0 [D loss: 0.814586, acc.: 52.13%] [G loss: 0.394912] [Loss change: 0.395, Loss increases: 0]\n",
      "Number of right predictions: 2435\n",
      "Number of wrong predictions: 2564\n",
      "Accuracy: 0.4871 \n",
      "50 [D loss: 0.753867, acc.: 53.14%] [G loss: 0.368416] [Loss change: 0.368, Loss increases: 0]\n",
      "Number of right predictions: 2574\n",
      "Number of wrong predictions: 2425\n",
      "Accuracy: 0.5149 \n",
      "100 [D loss: 0.746551, acc.: 54.88%] [G loss: 0.403224] [Loss change: 0.403, Loss increases: 0]\n",
      "Number of right predictions: 2141\n",
      "Number of wrong predictions: 2858\n",
      "Accuracy: 0.4283 \n",
      "150 [D loss: 0.731216, acc.: 58.98%] [G loss: 0.422336] [Loss change: 0.422, Loss increases: 0]\n",
      "Number of right predictions: 1370\n",
      "Number of wrong predictions: 3629\n",
      "Accuracy: 0.2741 \n",
      "200 [D loss: 0.773027, acc.: 58.19%] [G loss: 0.360494] [Loss change: 0.360, Loss increases: 0]\n",
      "Number of right predictions: 251\n",
      "Number of wrong predictions: 4748\n",
      "Accuracy: 0.0502 \n",
      "250 [D loss: 0.773454, acc.: 58.42%] [G loss: 0.438171] [Loss change: 0.438, Loss increases: 0]\n",
      "Number of right predictions: 553\n",
      "Number of wrong predictions: 4446\n",
      "Accuracy: 0.1106 \n",
      "300 [D loss: 0.764392, acc.: 57.97%] [G loss: 0.434212] [Loss change: 0.434, Loss increases: 0]\n",
      "Number of right predictions: 894\n",
      "Number of wrong predictions: 4105\n",
      "Accuracy: 0.1788 \n",
      "350 [D loss: 0.770612, acc.: 56.49%] [G loss: 0.426222] [Loss change: 0.426, Loss increases: 0]\n",
      "Number of right predictions: 1212\n",
      "Number of wrong predictions: 3787\n",
      "Accuracy: 0.2424 \n",
      "400 [D loss: 0.772550, acc.: 58.27%] [G loss: 0.457382] [Loss change: 0.457, Loss increases: 0]\n",
      "Number of right predictions: 1257\n",
      "Number of wrong predictions: 3742\n",
      "Accuracy: 0.2515 \n",
      "450 [D loss: 0.693817, acc.: 63.93%] [G loss: 0.448709] [Loss change: 0.449, Loss increases: 0]\n",
      "Number of right predictions: 1379\n",
      "Number of wrong predictions: 3620\n",
      "Accuracy: 0.2759 \n",
      "500 [D loss: 0.773719, acc.: 54.78%] [G loss: 0.433598] [Loss change: 0.434, Loss increases: 0]\n",
      "Number of right predictions: 1695\n",
      "Number of wrong predictions: 3304\n",
      "Accuracy: 0.3391 \n",
      "550 [D loss: 0.769774, acc.: 54.54%] [G loss: 0.418352] [Loss change: 0.418, Loss increases: 0]\n",
      "Number of right predictions: 1986\n",
      "Number of wrong predictions: 3013\n",
      "Accuracy: 0.3973 \n",
      "600 [D loss: 0.787588, acc.: 54.53%] [G loss: 0.402433] [Loss change: 0.402, Loss increases: 0]\n",
      "Number of right predictions: 2025\n",
      "Number of wrong predictions: 2974\n",
      "Accuracy: 0.4051 \n",
      "650 [D loss: 0.772936, acc.: 53.97%] [G loss: 0.406426] [Loss change: 0.406, Loss increases: 0]\n",
      "Number of right predictions: 2043\n",
      "Number of wrong predictions: 2956\n",
      "Accuracy: 0.4087 \n",
      "700 [D loss: 0.761217, acc.: 53.45%] [G loss: 0.416165] [Loss change: 0.416, Loss increases: 0]\n",
      "Number of right predictions: 2062\n",
      "Number of wrong predictions: 2937\n",
      "Accuracy: 0.4125 \n",
      "750 [D loss: 0.748179, acc.: 54.78%] [G loss: 0.422316] [Loss change: 0.422, Loss increases: 0]\n",
      "Number of right predictions: 1958\n",
      "Number of wrong predictions: 3041\n",
      "Accuracy: 0.3917 \n",
      "800 [D loss: 0.710652, acc.: 60.04%] [G loss: 0.407349] [Loss change: 0.407, Loss increases: 0]\n",
      "Number of right predictions: 1682\n",
      "Number of wrong predictions: 3317\n",
      "Accuracy: 0.3365 \n",
      "850 [D loss: 0.685325, acc.: 61.59%] [G loss: 0.290544] [Loss change: 0.291, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "900 [D loss: 0.754382, acc.: 55.15%] [G loss: 0.291381] [Loss change: 0.291, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "950 [D loss: 0.810014, acc.: 54.37%] [G loss: 0.411390] [Loss change: 0.411, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "1000 [D loss: 0.719879, acc.: 57.31%] [G loss: 0.477914] [Loss change: 0.478, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n"
=======
      "===============================================\n",
      "decoded:\n",
      "===============================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e3328f4aaeac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#training GAN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-16-ee563784d8f8>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(combined, discriminator, generator, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mdecode_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m  \u001b[0;31m# take a slice from the ndarray that we want to decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m#decode_test_df = pd.DataFrame(decode_test, columns=ll)  # turn that ndarray into a dataframe with correct column names and order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mdecoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# decode that dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'apply'"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "#training GAN model\n",
    "train_loop(combined, discriminator, generator, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
