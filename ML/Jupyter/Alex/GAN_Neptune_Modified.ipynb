{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41)) #discriminator takes 41 values from our dataset\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer2))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid')) #outputs 0 to 1, 1 being real and 0 being fake\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41))\n",
    "    #model.add(BatchNormalization())\n",
    "    #model.add(LeakyReLU(alpha=alpha))\n",
    "    #model.add(Dense(layer2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(41, activation='relu'))\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_model(layer1, layer2, layer3, alpha):\n",
    "    optimizer = Adam(0.001)\n",
    "    \n",
    "    #build generator and discriminator (mirrored)\n",
    "    generator = build_generator(layer1, layer2, layer3, alpha)\n",
    "    \n",
    "    discriminator = build_discriminator(layer3, layer2, layer1, alpha)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    #build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return combined, discriminator, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\", \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \n",
    "            \"hot\", \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\", \"num_file_creations\", \n",
    "            \"num_shells\", \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\", \n",
    "            \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\", \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \n",
    "            \"dst_host_srv_count\", \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\", \n",
    "            \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\", \n",
    "            \"dst_host_srv_rerror_rate\", \"attack_type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(combined, discriminator, generator, epochs):\n",
    "    epochs = epochs+1\n",
    "    batch_size = 4999\n",
    "    dataframe = pd.read_csv('../../../../CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    #dataframe = pd.read_csv('CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    \n",
    "    #apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(ll)\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    f = open(\"GANNeptune.txt\", \"a\")\n",
    "    np.savetxt(\"GANNeptune.txt\", dataset, fmt=\"%d\")\n",
    "    f.close()\n",
    "    \n",
    "#    d = defaultdict(LabelEncoder)\n",
    "#     fit = dataframe.apply(lambda x: d[x.name].fit_transform(x))  # fit is encoded dataframe\n",
    "#     dataset = fit.values   # transform to ndarray\n",
    "    #print(fit)\n",
    "    \n",
    "#     print(\"===============================================\")\n",
    "#     print(\"decoded:\")\n",
    "#     print(\"===============================================\")\n",
    "#     decode_test = dataframe_encoded  # take a slice from the ndarray that we want to decode\n",
    "#     #decode_test_df = pd.DataFrame(decode_test, columns=ll)  # turn that ndarray into a dataframe with correct column names and order\n",
    "#     decoded = decode_test.apply(le.inverse_transform)  # decode that dataframe\n",
    "#     print(decoded)\n",
    "    \n",
    "    #labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    #Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(int)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0\n",
    "    prev_g_loss = 0\n",
    "    \n",
    "    #generating a np array of numbers 0..batch_size-1\n",
    "    idx = np.arange(batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #selecting batch_size random attacks from our training data\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        #generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        #create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        #loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % \n",
    "                  (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "            #saving results to txt to track them as the gan is training\n",
    "            f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%d\")\n",
    "            f.close()\n",
    "            \n",
    "        \n",
    "            results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "            y_pred = estimator.predict(results)\n",
    "        \n",
    "            right = (y_pred == 1).sum()\n",
    "            wrong = len(y_pred)-(y_pred == 1).sum()\n",
    "            accuracy = (right/float(right+wrong))\n",
    "            print(\"Number of right predictions: %d\" % right)\n",
    "            print(\"Number of wrong predictions: %d\" % wrong)\n",
    "            print(\"Accuracy: %.4f \" % accuracy)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize Random Number Generator\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#load dataset\n",
    "dataframe = pd.read_csv(\"../../../../CSV/normalAndNeptune.csv\")\n",
    "#dataframe = pd.read_csv(\"CSV/normalAndNeptune.csv\")\n",
    "\n",
    "#samples n random data points\n",
    "dataframe = dataframe.sample(n=1000000)\n",
    "#LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "#apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(int)\n",
    "Y = dataset[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get validation data\n",
    "validationToTrainRatio = 0.05\n",
    "validationSize = int(validationToTrainRatio * len(X))\n",
    "validationData = X[:validationSize]\n",
    "validationLabels = Y[:validationSize]\n",
    "X = X[validationSize:]\n",
    "Y = Y[validationSize:]\n",
    "\n",
    "#Get test data\n",
    "testToTrainRatio = 0.05\n",
    "testSize = int(testToTrainRatio * len(X))\n",
    "testData = X[:testSize]\n",
    "testLabels = Y[:testSize]\n",
    "X = X[testSize:]\n",
    "Y = Y[testSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator = baseline_model(layers=2, units=32, dropout_rate=0.5, input_shape=X.shape[1:], num_classes=1)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = estimator.fit(X,\n",
    "                    Y,\n",
    "                    epochs=5,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(validationData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(estimator.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating GAN model\n",
    "combined, discriminator, generator = GAN_model(8, 16, 32, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#training GAN model\n",
    "train_loop(combined, discriminator, generator, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
