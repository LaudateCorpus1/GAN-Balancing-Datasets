{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/multi-class-classification-tutorial-keras-deep-learning-library/\n",
    "import numpy\n",
    "import pandas\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Random Number Generator\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# dataframe = pandas.read_csv(\"kddforpandatrain.csv\")#, header=True)\n",
    "dataframe = pandas.read_csv(\"kdd_dataset.csv\")#, header=True)  # read the whole 10% dataset into dataframe\n",
    "\n",
    "# samples 3000 random data points from 500k\n",
    "dataframe = dataframe.sample(n=3000)\n",
    "\n",
    "# LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(float)\n",
    "Y = dataset[:,41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "print(len(dummy_y[0]))\n",
    "num_of_classes = len(dummy_y[0])  # the length of dummy y is the number of classes we have in our small sample\n",
    "# since we are randomly sampling from a large dataset, we might not get 1 of every class in our sample\n",
    "# we need to set output layer to be equal to the length of our dummy_y vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model(offset1, offset2):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    global num_of_classes\n",
    "    \n",
    "    inputs = 41\n",
    "    hidden_layer1 = 10 + offset1\n",
    "    hidden_layer2 = 0 + offset2\n",
    "    hidden_layer3 = 0\n",
    "    outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "    \n",
    "    model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "    if hidden_layer2 != 0:\n",
    "        model.add(Dense(hidden_layer2, activation='relu'))\n",
    "    if hidden_layer3 != 0:\n",
    "        model.add(Dense(hidden_layer3, activation='relu'))\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset1 = 0\n",
    "offset2 = 0\n",
    "# wait this is totally unnecessary because I figured out how to do global variables in python lmao woops\n",
    "def wrapper(): # wrapper class, because keras needs a class that just returns the model\n",
    "    global offset1\n",
    "    global offset2\n",
    "    # print(str(offset1) + \"   \" + str(offset2))\n",
    "    model = baseline_model(offset1, offset2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden Layer 1: 10, Hidden Layer 2: 0, Output classes: 12, Baseline: 53.17% (22.01%)\n"
     ]
    }
   ],
   "source": [
    "# checks every combination of hidden 1: 10 to 20 and hidden 2: 0 to 10 \n",
    "for i in range(0, 10):\n",
    "    offset1 = i\n",
    "    for j in range(0, 10):\n",
    "        offset2 = j\n",
    "        estimator = KerasClassifier(build_fn=wrapper, epochs=200, batch_size=5, verbose=0)\n",
    "        kfold = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "        results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "        results_string = \"Hidden Layer 1: \" + str(10 + offset1) + \", Hidden Layer 2: \" + str(offset2) + \", Output classes: \" + str(num_of_classes) + \", Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100)\n",
    "        f = open('KerasOptimizationResults.txt','a+')\n",
    "        f.write(\"\\n\" + results_string)\n",
    "        f.close()\n",
    "        print(results_string)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
