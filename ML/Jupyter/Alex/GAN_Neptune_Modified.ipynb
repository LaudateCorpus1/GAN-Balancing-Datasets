{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41)) #discriminator takes 41 values from our dataset\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer2))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid')) #outputs 0 to 1, 1 being real and 0 being fake\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(layer1, layer2, layer3, alpha):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(layer2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=alpha))\n",
    "    model.add(Dense(41, activation='relu'))\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_model(layer1, layer2, layer3, alpha):\n",
    "    optimizer = Adam(0.001)\n",
    "    \n",
    "    #build generator and discriminator (mirrored)\n",
    "    generator = build_generator(layer1, layer2, layer3, alpha)\n",
    "    \n",
    "    discriminator = build_discriminator(layer3, layer2, layer1, alpha)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    #build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return combined, discriminator, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(combined, discriminator, generator, epochs):\n",
    "    epochs = epochs+1\n",
    "    batch_size = 4999\n",
    "    #dataframe = pd.read_csv('../../../../CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    dataframe = pd.read_csv('CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    \n",
    "    #apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    #labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    #Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(int)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0\n",
    "    prev_g_loss = 0\n",
    "    \n",
    "    #generating a np array of numbers 0..batch_size-1\n",
    "    idx = np.arange(batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #selecting batch_size random attacks from our training data\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        #generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        #create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        #loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % \n",
    "                  (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "            #saving results to txt to track them as the gan is training\n",
    "            f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%d\")\n",
    "            f.close()\n",
    "        \n",
    "            results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "            y_pred = estimator.predict(results)\n",
    "        \n",
    "            right = (y_pred == 1).sum()\n",
    "            wrong = len(y_pred)-(y_pred == 1).sum()\n",
    "            accuracy = (right/float(right+wrong))\n",
    "            print(\"Number of right predictions: %d\" % right)\n",
    "            print(\"Number of wrong predictions: %d\" % wrong)\n",
    "            print(\"Accuracy: %.4f \" % accuracy)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Initialize Random Number Generator\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#load dataset\n",
    "#dataframe = pd.read_csv(\"../../../../CSV/normalAndNeptune.csv\")\n",
    "dataframe = pd.read_csv(\"CSV/normalAndNeptune.csv\")\n",
    "\n",
    "#samples n random data points\n",
    "dataframe = dataframe.sample(n=1000000)\n",
    "#LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "#apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(int)\n",
    "Y = dataset[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get validation data\n",
    "validationToTrainRatio = 0.05\n",
    "validationSize = int(validationToTrainRatio * len(X))\n",
    "validationData = X[:validationSize]\n",
    "validationLabels = Y[:validationSize]\n",
    "X = X[validationSize:]\n",
    "Y = Y[validationSize:]\n",
    "\n",
    "#Get test data\n",
    "testToTrainRatio = 0.05\n",
    "testSize = int(testToTrainRatio * len(X))\n",
    "testData = X[:testSize]\n",
    "testLabels = Y[:testSize]\n",
    "X = X[testSize:]\n",
    "Y = Y[testSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation='relu'))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation='sigmoid'))\n",
    "    model.compile(optimizer=Adam(0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0228 03:56:43.457408 140519197656832 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 902500 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      " - 8s - loss: 0.5772 - acc: 0.9310 - val_loss: 0.0044 - val_acc: 0.9999\n",
      "Epoch 2/5\n",
      " - 7s - loss: 0.0489 - acc: 0.9859 - val_loss: 0.0063 - val_acc: 0.9998\n",
      "Epoch 3/5\n",
      " - 6s - loss: 0.0336 - acc: 0.9897 - val_loss: 0.0030 - val_acc: 0.9999\n",
      "Epoch 4/5\n",
      " - 6s - loss: 0.0279 - acc: 0.9909 - val_loss: 0.0014 - val_acc: 0.9999\n",
      "Epoch 5/5\n",
      " - 6s - loss: 0.0247 - acc: 0.9917 - val_loss: 0.0011 - val_acc: 1.0000\n"
     ]
    }
   ],
>>>>>>> 58febd09a1b9b9678eece306c56fcb729cb30f09
   "source": [
    "estimator = baseline_model(layers=2, units=32, dropout_rate=0.5, input_shape=X.shape[1:], num_classes=1)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = estimator.fit(X,\n",
    "                    Y,\n",
    "                    epochs=5,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(validationData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47500/47500 [==============================] - 1s 25us/step\n",
      "[0.001603735167716973, 0.9998736842105264]\n"
     ]
    }
   ],
>>>>>>> 58febd09a1b9b9678eece306c56fcb729cb30f09
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(estimator.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0228 03:57:19.265904 140519197656832 deprecation.py:506] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/training/moving_averages.py:210: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#creating GAN model\n",
    "combined, discriminator, generator = GAN_model(8, 16, 32, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
<<<<<<< HEAD
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 2.643809, acc.: 49.04%] [G loss: 0.778235] [Loss change: 0.778, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "50 [D loss: 0.906326, acc.: 50.17%] [G loss: 0.251075] [Loss change: 0.251, Loss increases: 0]\n",
      "Number of right predictions: 21\n",
      "Number of wrong predictions: 4978\n",
      "Accuracy: 0.0042 \n",
      "100 [D loss: 1.141420, acc.: 50.19%] [G loss: 0.157663] [Loss change: 0.158, Loss increases: 0]\n",
      "Number of right predictions: 15\n",
      "Number of wrong predictions: 4984\n",
      "Accuracy: 0.0030 \n",
      "150 [D loss: 1.300450, acc.: 50.32%] [G loss: 0.134700] [Loss change: 0.135, Loss increases: 0]\n",
      "Number of right predictions: 6\n",
      "Number of wrong predictions: 4993\n",
      "Accuracy: 0.0012 \n",
      "200 [D loss: 1.449427, acc.: 50.57%] [G loss: 0.120876] [Loss change: 0.121, Loss increases: 0]\n",
      "Number of right predictions: 1\n",
      "Number of wrong predictions: 4998\n",
      "Accuracy: 0.0002 \n",
      "250 [D loss: 1.500217, acc.: 50.53%] [G loss: 0.124885] [Loss change: 0.125, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "300 [D loss: 1.519135, acc.: 50.57%] [G loss: 0.122630] [Loss change: 0.123, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n"
     ]
    }
   ],
>>>>>>> 58febd09a1b9b9678eece306c56fcb729cb30f09
   "source": [
    "#training GAN model\n",
    "train_loop(combined, discriminator, generator, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
