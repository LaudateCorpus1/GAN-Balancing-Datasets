{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(41, input_dim=41, activation='relu'))  # discriminator takes 41 values from our dataset\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(15, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # outputs 0 to 1, 1 being read and 0 being fake\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(hidden1, hidden2, hidden3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden1, input_dim=41))  # arbitrarily selected 100 for our input noise vector?\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(hidden3))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(41, activation='relu'))  # outputs a generated vector of the same size as our data (41)\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(gen_hidden1, gen_hidden2, gen_hidden3):\n",
    "    batch_size = 256\n",
    "    epochs = 7000\n",
    "    optimizer = Adam(0.0002, 0.5)\n",
    "    \n",
    "    dataframe = pd.read_csv('smurfOnly.csv').sample(500) # sample 100 data points randomly from the csv\n",
    "    \n",
    "    # apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    #to visually judge results\n",
    "    print(\"Real smurf attacks:\")\n",
    "    print(dataset[:2])\n",
    "    \n",
    "    # Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(float)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    # labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # build the discriminator portion\n",
    "    discriminator = build_discriminator();\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # build the generator portion\n",
    "    generator = build_generator(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    # build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0;\n",
    "    prev_g_loss = 0;\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # selecting batch_size random attacks from our training data\n",
    "        idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        # generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        # create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        # loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 100 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "        # if our generator loss icreased this iteration, increment the counter by 1\n",
    "        if (g_loss - prev_g_loss) > 0:\n",
    "            loss_increase_count = loss_increase_count + 1\n",
    "        else: \n",
    "            loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "            \n",
    "        prev_g_loss = g_loss\n",
    "            \n",
    "        if loss_increase_count > 5:\n",
    "            print('Stoping on iteration: ', epoch)\n",
    "            break\n",
    "            \n",
    "        if epoch % 20 == 0:\n",
    "            f = open(\"GANresultsSmurf.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsSmurf.txt\", gen_attacks, fmt=\"%.0f\")\n",
    "            f.close()\n",
    "            \n",
    "    # peek at our results\n",
    "    results = np.loadtxt(\"GANresultsSmurf.txt\")\n",
    "    print(\"Generated Smurf attacks: \")\n",
    "    print(results[:2])\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['normal.' 'smurf.']\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Number Generator\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "\n",
    "dataframe = pd.read_csv(\"smurfAndNormal.csv\")#, header=True) \n",
    "\n",
    "# samples 10000 random data points from 500k\n",
    "dataframe = dataframe.sample(n=15000)\n",
    "# LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_smurf = np.where(attack_labels == 'smurf.')\n",
    "smurf_index = indices_of_smurf[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "print(attack_labels)\n",
    "print(smurf_index)\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(float)\n",
    "Y = dataset[:,41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "#print(len(dummy_y[0]))\n",
    "num_of_classes = len(dummy_y[0])  # the length of dummy y is the number of classes we have in our small sample\n",
    "# since we are randomly sampling from a large dataset, we might not get 1 of every class in our sample\n",
    "# we need to set output layer to be equal to the length of our dummy_y vectors\n",
    "print(num_of_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    \n",
    "    inputs = 41\n",
    "    hidden_layer1 = 10\n",
    "    hidden_layer2 = 5\n",
    "    hidden_layer3 = 0\n",
    "    outputs = num_of_classes  #needs to be this variable in case we forget to sample. Could end up having 10 classes or 12, etc\n",
    "    \n",
    "    model.add(Dense(hidden_layer1, input_dim=inputs, activation='relu'))\n",
    "    if hidden_layer2 != 0:\n",
    "        model.add(Dense(hidden_layer2, activation='relu'))\n",
    "    if hidden_layer3 != 0:\n",
    "        model.add(Dense(hidden_layer3, activation='relu'))\n",
    "    model.add(Dense(outputs, activation='softmax'))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) #optimizer=adam\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n",
      " - 0s - loss: 1.2883 - acc: 0.8981\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.3875 - acc: 0.9749\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.2963 - acc: 0.9804\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0531 - acc: 0.9954\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0031 - acc: 0.9995\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9999\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9999\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 8.9904e-04 - acc: 0.9998\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0010 - acc: 0.9999\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9999\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9999\n",
      "Epoch 1/32\n",
      " - 0s - loss: 0.4847 - acc: 0.8512\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.0404 - acc: 0.9901\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0161 - acc: 0.9977\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0110 - acc: 0.9985\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0088 - acc: 0.9987\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0073 - acc: 0.9991\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0064 - acc: 0.9992\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9992\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0040 - acc: 0.9994\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0036 - acc: 0.9996\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0031 - acc: 0.9996\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0029 - acc: 0.9996\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9996\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 1/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9465 - acc: 0.2588\n",
      "Epoch 1/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9513 - acc: 0.2585\n",
      "Epoch 1/32\n",
      " - 0s - loss: 11.9591 - acc: 0.2579\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9586 - acc: 0.2580\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9585 - acc: 0.2581\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9585 - acc: 0.2581\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9585 - acc: 0.2581\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9585 - acc: 0.2581\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9584 - acc: 0.2581\n",
      "Epoch 1/32\n",
      " - 0s - loss: 12.1395 - acc: 0.2353\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9171 - acc: 0.2606\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9169 - acc: 0.2607\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9168 - acc: 0.2607\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9168 - acc: 0.2607\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 14/32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9167 - acc: 0.2607\n",
      "Epoch 1/32\n",
      " - 0s - loss: 12.4745 - acc: 0.2199\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9274 - acc: 0.2600\n",
      "Epoch 1/32\n",
      " - 0s - loss: 3.5085 - acc: 0.7756\n",
      "Epoch 2/32\n",
      " - 0s - loss: 1.2077 - acc: 0.9159\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.2704 - acc: 0.9809\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0077 - acc: 0.9990\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9995\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9999\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9999\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 20/32\n",
      " - 0s - loss: 9.2369e-04 - acc: 0.9999\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 1/32\n",
      " - 0s - loss: 12.0999 - acc: 0.2481\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.8893 - acc: 0.2624\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.8892 - acc: 0.2624\n",
      "Epoch 1/32\n",
      " - 0s - loss: 12.0197 - acc: 0.2516\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9217 - acc: 0.2604\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9216 - acc: 0.2604\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9216 - acc: 0.2604\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 1/32\n",
      " - 0s - loss: 3.2989 - acc: 0.7872\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.2525 - acc: 0.9773\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0093 - acc: 0.9984\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0063 - acc: 0.9995\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0063 - acc: 0.9995\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0059 - acc: 0.9996\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0062 - acc: 0.9995\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0058 - acc: 0.9995\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9996\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9994\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9996\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0050 - acc: 0.9996\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0046 - acc: 0.9997\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0042 - acc: 0.9996\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0041 - acc: 0.9996\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0031 - acc: 0.9998\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9998\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9998\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9998\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9999\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9999\n",
      "Epoch 1/32\n",
      " - 0s - loss: 11.9221 - acc: 0.2603\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9216 - acc: 0.2604\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9216 - acc: 0.2604\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 1/32\n",
      " - 0s - loss: 3.4564 - acc: 0.7725\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.3243 - acc: 0.9750\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0843 - acc: 0.9916\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0455 - acc: 0.9941\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0195 - acc: 0.9967\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0095 - acc: 0.9982\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0072 - acc: 0.9991\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0063 - acc: 0.9993\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0058 - acc: 0.9995\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0049 - acc: 0.9996\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0041 - acc: 0.9996\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9997\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9997\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9997\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 1/32\n",
      " - 0s - loss: 1.9948 - acc: 0.7656\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.1418 - acc: 0.9890\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0920 - acc: 0.9968\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0683 - acc: 0.9976\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0149 - acc: 0.9985\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0115 - acc: 0.9987\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0106 - acc: 0.9989\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0099 - acc: 0.9990\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0094 - acc: 0.9990\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0087 - acc: 0.9992\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0081 - acc: 0.9992\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0073 - acc: 0.9993\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0069 - acc: 0.9993\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0067 - acc: 0.9993\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0065 - acc: 0.9993\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0063 - acc: 0.9993\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0060 - acc: 0.9995\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0058 - acc: 0.9995\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0055 - acc: 0.9995\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0051 - acc: 0.9996\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0048 - acc: 0.9996\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0046 - acc: 0.9996\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0045 - acc: 0.9996\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0042 - acc: 0.9996\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0040 - acc: 0.9996\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0038 - acc: 0.9996\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0037 - acc: 0.9996\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0034 - acc: 0.9996\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0035 - acc: 0.9996\n",
      "Epoch 1/32\n",
      " - 1s - loss: 2.1248 - acc: 0.8190\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.0170 - acc: 0.9981\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0081 - acc: 0.9990\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0057 - acc: 0.9993\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0045 - acc: 0.9993\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0039 - acc: 0.9993\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0034 - acc: 0.9994\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0033 - acc: 0.9996\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0032 - acc: 0.9994\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0029 - acc: 0.9994\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9995\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9995\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9996\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9996\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9996\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9997\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9997\n",
      "Epoch 1/32\n",
      " - 1s - loss: 0.0412 - acc: 0.9949\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.0045 - acc: 0.9992\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0028 - acc: 0.9996\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9994\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9996\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9997\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9996\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0030 - acc: 0.9996\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0029 - acc: 0.9997\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9995\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0028 - acc: 0.9997\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9997\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0018 - acc: 0.9998\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 30/32\n",
      " - 0s - loss: 6.8527e-04 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9998\n",
      "Epoch 1/32\n",
      " - 1s - loss: 0.2502 - acc: 0.9696\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.0140 - acc: 0.9967\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0117 - acc: 0.9973\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0109 - acc: 0.9981\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0102 - acc: 0.9982\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0095 - acc: 0.9984\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0086 - acc: 0.9986\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0084 - acc: 0.9988\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0083 - acc: 0.9989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/32\n",
      " - 0s - loss: 0.0081 - acc: 0.9986\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0081 - acc: 0.9990\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0075 - acc: 0.9988\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0072 - acc: 0.9989\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0067 - acc: 0.9990\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0059 - acc: 0.9990\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9990\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0048 - acc: 0.9990\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0045 - acc: 0.9991\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0039 - acc: 0.9995\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0038 - acc: 0.9995\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0032 - acc: 0.9995\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9995\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9996\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9996\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9997\n",
      "Epoch 1/32\n",
      " - 1s - loss: 11.9125 - acc: 0.2607\n",
      "Epoch 2/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 3/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9119 - acc: 0.2610\n",
      "Epoch 1/32\n",
      " - 1s - loss: 1.3884 - acc: 0.8766\n",
      "Epoch 2/32\n",
      " - 0s - loss: 0.0557 - acc: 0.9813\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.0409 - acc: 0.9928\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0353 - acc: 0.9946\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0243 - acc: 0.9963\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0158 - acc: 0.9969\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0141 - acc: 0.9978\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0129 - acc: 0.9984\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0122 - acc: 0.9986\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0115 - acc: 0.9987\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0108 - acc: 0.9989\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0103 - acc: 0.9990\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0098 - acc: 0.9990\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0093 - acc: 0.9994\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0090 - acc: 0.9995\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0086 - acc: 0.9994\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0081 - acc: 0.9994\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0078 - acc: 0.9995\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0075 - acc: 0.9997\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0072 - acc: 0.9997\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0068 - acc: 0.9996\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0062 - acc: 0.9997\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0060 - acc: 0.9997\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0043 - acc: 0.9998\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0027 - acc: 0.9997\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0026 - acc: 0.9997\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9998\n",
      "Epoch 28/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9997\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9998\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0020 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 1/32\n",
      " - 1s - loss: 14.1918 - acc: 0.1148\n",
      "Epoch 2/32\n",
      " - 0s - loss: 13.6053 - acc: 0.1520\n",
      "Epoch 3/32\n",
      " - 0s - loss: 12.0749 - acc: 0.2476\n",
      "Epoch 4/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 5/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 6/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 7/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 8/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 9/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 10/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 11/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 12/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 13/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 14/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 15/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 16/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 17/32\n",
      " - 0s - loss: 11.9215 - acc: 0.2604\n",
      "Epoch 18/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 19/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 20/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 21/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 22/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 23/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 24/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 25/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 26/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 27/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 28/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 29/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 30/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 31/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 32/32\n",
      " - 0s - loss: 11.9214 - acc: 0.2604\n",
      "Epoch 1/32\n",
      " - 1s - loss: 4.1753 - acc: 0.7402\n",
      "Epoch 2/32\n",
      " - 0s - loss: 3.0163 - acc: 0.7973\n",
      "Epoch 3/32\n",
      " - 0s - loss: 0.1092 - acc: 0.9873\n",
      "Epoch 4/32\n",
      " - 0s - loss: 0.0074 - acc: 0.9981\n",
      "Epoch 5/32\n",
      " - 0s - loss: 0.0054 - acc: 0.9984\n",
      "Epoch 6/32\n",
      " - 0s - loss: 0.0038 - acc: 0.9989\n",
      "Epoch 7/32\n",
      " - 0s - loss: 0.0031 - acc: 0.9992\n",
      "Epoch 8/32\n",
      " - 0s - loss: 0.0028 - acc: 0.9994\n",
      "Epoch 9/32\n",
      " - 0s - loss: 0.0025 - acc: 0.9996\n",
      "Epoch 10/32\n",
      " - 0s - loss: 0.0024 - acc: 0.9996\n",
      "Epoch 11/32\n",
      " - 0s - loss: 0.0023 - acc: 0.9996\n",
      "Epoch 12/32\n",
      " - 0s - loss: 0.0022 - acc: 0.9996\n",
      "Epoch 13/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 14/32\n",
      " - 0s - loss: 0.0021 - acc: 0.9997\n",
      "Epoch 15/32\n",
      " - 0s - loss: 0.0019 - acc: 0.9998\n",
      "Epoch 16/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 17/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 18/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 19/32\n",
      " - 0s - loss: 0.0017 - acc: 0.9998\n",
      "Epoch 20/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 21/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 22/32\n",
      " - 0s - loss: 0.0015 - acc: 0.9998\n",
      "Epoch 23/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 24/32\n",
      " - 0s - loss: 0.0014 - acc: 0.9998\n",
      "Epoch 25/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 26/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 27/32\n",
      " - 0s - loss: 0.0013 - acc: 0.9998\n",
      "Epoch 28/32\n",
      " - 0s - loss: 9.7105e-04 - acc: 0.9998\n",
      "Epoch 29/32\n",
      " - 0s - loss: 0.0012 - acc: 0.9998\n",
      "Epoch 30/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9998\n",
      "Epoch 31/32\n",
      " - 0s - loss: 0.0016 - acc: 0.9998\n",
      "Epoch 32/32\n",
      " - 0s - loss: 0.0011 - acc: 0.9998\n",
      "<class 'keras.wrappers.scikit_learn.KerasClassifier'>\n",
      "[[3897    2]\n",
      " [7761 3340]]\n",
      "total: 15000\n",
      "accuracy: 0.48246666666666665\n",
      "Matthews correlation coefficient: 0.31658164788580295\n",
      "Baseline: 77.60% (34.16%)\n"
     ]
    }
   ],
   "source": [
    "#for i in range(0,10):\n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=32, batch_size=200, verbose=2)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "y_pred = cross_val_predict(estimator, X, dummy_y, cv=kfold)\n",
    "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
    "\n",
    "trained_classifier = estimator.fit(X, Y)\n",
    "print(type(estimator))\n",
    "\n",
    "cm = confusion_matrix(Y, y_pred)\n",
    "print(cm)\n",
    "print(\"total: \" + str(cm.sum()))\n",
    "print(\"accuracy: \" + str(np.trace(cm) / cm.sum()))\n",
    "print(\"Matthews correlation coefficient: \" + str(matthews_corrcoef(Y, y_pred)))\n",
    "\n",
    "\n",
    "\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n",
    "f = open(\"discriminatorResults.txt\", \"a+\")\n",
    "f.write(\"TP: %d, FP: %d, FN: %d, TN: %d\\n\" % (cm[0][0], cm[0][1], cm[1][0], cm[1][1]))\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34 34\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34 34\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 1.369914, acc.: 49.41%] [G loss: 0.832706] [Loss change: 0.833, Loss increases: 0]\n",
      "100 [D loss: 0.481357, acc.: 53.32%] [G loss: 0.507623] [Loss change: 0.002, Loss increases: 0]\n",
      "200 [D loss: 0.698590, acc.: 50.98%] [G loss: 0.346337] [Loss change: 0.003, Loss increases: 1]\n",
      "300 [D loss: 0.805833, acc.: 51.95%] [G loss: 0.301139] [Loss change: -0.019, Loss increases: 1]\n",
      "400 [D loss: 0.807378, acc.: 52.34%] [G loss: 0.283261] [Loss change: 0.003, Loss increases: 1]\n",
      "500 [D loss: 0.868982, acc.: 50.78%] [G loss: 0.265436] [Loss change: -0.009, Loss increases: 0]\n",
      "600 [D loss: 0.851985, acc.: 52.34%] [G loss: 0.272886] [Loss change: 0.006, Loss increases: 1]\n",
      "700 [D loss: 0.889143, acc.: 51.17%] [G loss: 0.247382] [Loss change: -0.019, Loss increases: 0]\n",
      "800 [D loss: 0.849121, acc.: 51.56%] [G loss: 0.258569] [Loss change: -0.003, Loss increases: 1]\n",
      "900 [D loss: 0.822889, acc.: 52.15%] [G loss: 0.274002] [Loss change: 0.011, Loss increases: 0]\n",
      "1000 [D loss: 0.798333, acc.: 51.37%] [G loss: 0.275198] [Loss change: 0.010, Loss increases: 0]\n",
      "1100 [D loss: 0.827368, acc.: 51.37%] [G loss: 0.275608] [Loss change: -0.001, Loss increases: 2]\n",
      "1200 [D loss: 0.769298, acc.: 51.76%] [G loss: 0.287719] [Loss change: 0.016, Loss increases: 0]\n",
      "1300 [D loss: 0.795034, acc.: 50.98%] [G loss: 0.280325] [Loss change: -0.012, Loss increases: 0]\n",
      "1400 [D loss: 0.746358, acc.: 51.56%] [G loss: 0.300401] [Loss change: 0.009, Loss increases: 0]\n",
      "1500 [D loss: 0.728465, acc.: 51.56%] [G loss: 0.311697] [Loss change: -0.003, Loss increases: 1]\n",
      "1600 [D loss: 0.721529, acc.: 51.76%] [G loss: 0.310941] [Loss change: -0.007, Loss increases: 2]\n",
      "1700 [D loss: 0.725806, acc.: 50.98%] [G loss: 0.317377] [Loss change: 0.002, Loss increases: 0]\n",
      "1800 [D loss: 0.676929, acc.: 51.95%] [G loss: 0.338313] [Loss change: 0.004, Loss increases: 0]\n",
      "1900 [D loss: 0.676708, acc.: 51.17%] [G loss: 0.335600] [Loss change: 0.001, Loss increases: 0]\n",
      "2000 [D loss: 0.665305, acc.: 51.95%] [G loss: 0.347037] [Loss change: -0.008, Loss increases: 0]\n",
      "2100 [D loss: 0.643201, acc.: 51.95%] [G loss: 0.373329] [Loss change: 0.004, Loss increases: 0]\n",
      "2200 [D loss: 0.629446, acc.: 51.95%] [G loss: 0.385050] [Loss change: -0.000, Loss increases: 1]\n",
      "2300 [D loss: 0.613014, acc.: 52.34%] [G loss: 0.398851] [Loss change: 0.009, Loss increases: 0]\n",
      "2400 [D loss: 0.601802, acc.: 52.34%] [G loss: 0.406595] [Loss change: 0.004, Loss increases: 0]\n",
      "2500 [D loss: 0.581638, acc.: 53.12%] [G loss: 0.428371] [Loss change: -0.001, Loss increases: 1]\n",
      "2600 [D loss: 0.564420, acc.: 55.08%] [G loss: 0.446719] [Loss change: 0.014, Loss increases: 0]\n",
      "2700 [D loss: 0.554488, acc.: 53.12%] [G loss: 0.454567] [Loss change: -0.012, Loss increases: 1]\n",
      "2800 [D loss: 0.551013, acc.: 52.34%] [G loss: 0.460455] [Loss change: -0.013, Loss increases: 0]\n",
      "2900 [D loss: 0.524748, acc.: 54.30%] [G loss: 0.496146] [Loss change: 0.005, Loss increases: 1]\n",
      "3000 [D loss: 0.519939, acc.: 55.08%] [G loss: 0.510853] [Loss change: -0.003, Loss increases: 1]\n",
      "3100 [D loss: 0.496320, acc.: 55.66%] [G loss: 0.534504] [Loss change: 0.004, Loss increases: 0]\n",
      "3200 [D loss: 0.471646, acc.: 55.66%] [G loss: 0.559696] [Loss change: 0.006, Loss increases: 1]\n",
      "3300 [D loss: 0.466456, acc.: 55.27%] [G loss: 0.582146] [Loss change: 0.010, Loss increases: 1]\n",
      "3400 [D loss: 0.444333, acc.: 58.40%] [G loss: 0.594363] [Loss change: 0.004, Loss increases: 1]\n",
      "3500 [D loss: 0.442246, acc.: 57.23%] [G loss: 0.616020] [Loss change: -0.007, Loss increases: 2]\n",
      "3600 [D loss: 0.429973, acc.: 57.62%] [G loss: 0.640266] [Loss change: 0.009, Loss increases: 0]\n",
      "3700 [D loss: 0.431350, acc.: 65.82%] [G loss: 0.644577] [Loss change: -0.018, Loss increases: 1]\n",
      "3800 [D loss: 0.423435, acc.: 66.21%] [G loss: 0.677069] [Loss change: 0.007, Loss increases: 0]\n",
      "3900 [D loss: 0.385830, acc.: 71.09%] [G loss: 0.704181] [Loss change: -0.036, Loss increases: 1]\n",
      "4000 [D loss: 0.416655, acc.: 66.60%] [G loss: 0.731206] [Loss change: 0.004, Loss increases: 1]\n",
      "4100 [D loss: 0.373217, acc.: 71.48%] [G loss: 0.779787] [Loss change: 0.021, Loss increases: 0]\n",
      "4200 [D loss: 0.361597, acc.: 74.61%] [G loss: 0.799536] [Loss change: 0.014, Loss increases: 0]\n",
      "4300 [D loss: 0.398113, acc.: 75.59%] [G loss: 0.799634] [Loss change: -0.019, Loss increases: 1]\n",
      "4400 [D loss: 0.410076, acc.: 89.45%] [G loss: 0.821903] [Loss change: -0.014, Loss increases: 0]\n",
      "4500 [D loss: 0.357887, acc.: 90.04%] [G loss: 0.887711] [Loss change: 0.022, Loss increases: 0]\n",
      "4600 [D loss: 0.375492, acc.: 84.77%] [G loss: 0.936058] [Loss change: 0.015, Loss increases: 0]\n",
      "4700 [D loss: 0.333989, acc.: 81.84%] [G loss: 0.902451] [Loss change: 0.021, Loss increases: 1]\n",
      "4800 [D loss: 0.398175, acc.: 73.24%] [G loss: 0.799991] [Loss change: -0.010, Loss increases: 2]\n",
      "4900 [D loss: 0.407715, acc.: 72.46%] [G loss: 0.758321] [Loss change: -0.018, Loss increases: 1]\n",
      "5000 [D loss: 0.407840, acc.: 70.90%] [G loss: 0.773653] [Loss change: -0.003, Loss increases: 0]\n",
      "5100 [D loss: 0.397707, acc.: 74.22%] [G loss: 0.797461] [Loss change: -0.004, Loss increases: 2]\n",
      "5200 [D loss: 0.393899, acc.: 72.07%] [G loss: 0.805566] [Loss change: 0.017, Loss increases: 0]\n",
      "5300 [D loss: 0.417512, acc.: 71.48%] [G loss: 0.789806] [Loss change: 0.012, Loss increases: 1]\n",
      "5400 [D loss: 0.436238, acc.: 67.19%] [G loss: 0.792421] [Loss change: -0.006, Loss increases: 2]\n",
      "5500 [D loss: 0.445378, acc.: 68.55%] [G loss: 0.766633] [Loss change: -0.018, Loss increases: 0]\n",
      "5600 [D loss: 0.455880, acc.: 68.95%] [G loss: 0.760045] [Loss change: -0.042, Loss increases: 0]\n",
      "5700 [D loss: 0.467681, acc.: 66.99%] [G loss: 0.766927] [Loss change: -0.006, Loss increases: 0]\n",
      "5800 [D loss: 0.481191, acc.: 67.19%] [G loss: 0.804788] [Loss change: 0.043, Loss increases: 0]\n",
      "5900 [D loss: 0.500527, acc.: 66.80%] [G loss: 0.766397] [Loss change: 0.011, Loss increases: 0]\n",
      "6000 [D loss: 0.507323, acc.: 66.80%] [G loss: 0.744875] [Loss change: -0.003, Loss increases: 0]\n",
      "6100 [D loss: 0.524930, acc.: 65.23%] [G loss: 0.749967] [Loss change: -0.025, Loss increases: 1]\n",
      "6200 [D loss: 0.531290, acc.: 67.77%] [G loss: 0.756062] [Loss change: -0.004, Loss increases: 1]\n",
      "6300 [D loss: 0.521463, acc.: 69.34%] [G loss: 0.768936] [Loss change: 0.014, Loss increases: 1]\n",
      "6400 [D loss: 0.515667, acc.: 68.95%] [G loss: 0.749345] [Loss change: 0.009, Loss increases: 0]\n",
      "6500 [D loss: 0.567686, acc.: 66.21%] [G loss: 0.748005] [Loss change: -0.001, Loss increases: 0]\n",
      "6600 [D loss: 0.566918, acc.: 66.02%] [G loss: 0.721043] [Loss change: 0.009, Loss increases: 0]\n",
      "6700 [D loss: 0.597602, acc.: 64.65%] [G loss: 0.707574] [Loss change: -0.008, Loss increases: 1]\n",
      "6800 [D loss: 0.596332, acc.: 63.87%] [G loss: 0.708965] [Loss change: -0.010, Loss increases: 1]\n",
      "6900 [D loss: 0.629205, acc.: 62.50%] [G loss: 0.697925] [Loss change: -0.003, Loss increases: 1]\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 25. 26.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 0 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1 0 1 1 1\n",
      " 1 1 1 0 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1 1 1 0 1 0 1 0 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 1 1 0 1 1]\n",
      "Attack type: normal.     number predicted:  62\n",
      "Attack type: smurf.     number predicted:  194\n",
      "\n",
      "[[  0   0]\n",
      " [ 62 194]]\n",
      "total: 256\n",
      "accuracy: 0.7578125\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34 34\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 34 34\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 2.000813, acc.: 5.66%] [G loss: 0.655410] [Loss change: 0.655, Loss increases: 0]\n",
      "100 [D loss: 0.680956, acc.: 50.00%] [G loss: 0.342085] [Loss change: 0.002, Loss increases: 0]\n",
      "200 [D loss: 0.870636, acc.: 50.00%] [G loss: 0.243498] [Loss change: 0.004, Loss increases: 1]\n",
      "300 [D loss: 0.903877, acc.: 50.00%] [G loss: 0.215737] [Loss change: -0.006, Loss increases: 0]\n",
      "400 [D loss: 1.025675, acc.: 50.00%] [G loss: 0.189370] [Loss change: -0.006, Loss increases: 1]\n",
      "500 [D loss: 1.082617, acc.: 50.00%] [G loss: 0.168070] [Loss change: -0.006, Loss increases: 0]\n",
      "600 [D loss: 1.105909, acc.: 50.00%] [G loss: 0.162318] [Loss change: 0.001, Loss increases: 0]\n",
      "700 [D loss: 1.115358, acc.: 50.20%] [G loss: 0.154664] [Loss change: 0.007, Loss increases: 0]\n",
      "800 [D loss: 1.133312, acc.: 49.61%] [G loss: 0.145472] [Loss change: -0.008, Loss increases: 1]\n",
      "900 [D loss: 1.075602, acc.: 50.59%] [G loss: 0.146471] [Loss change: -0.000, Loss increases: 0]\n",
      "Stoping on iteration:  984\n",
      "Generated Smurf attacks: \n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5.\n",
      "  0. 0. 0. 2. 1. 1. 0. 0. 0. 0. 0. 0. 3. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 4.\n",
      "  1. 0. 0. 3. 1. 2. 0. 0. 0. 0. 1. 0. 5. 0. 0. 0. 0.]]\n",
      "[0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 1 0 0 0\n",
      " 0 0 0 1 1 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0\n",
      " 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 1 1 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1]\n",
      "Attack type: normal.     number predicted:  178\n",
      "Attack type: smurf.     number predicted:  78\n",
      "\n",
      "[[  0   0]\n",
      " [178  78]]\n",
      "total: 256\n",
      "accuracy: 0.3046875\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 29\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.817933, acc.: 0.20%] [G loss: 0.575956] [Loss change: 0.576, Loss increases: 0]\n",
      "100 [D loss: 0.637558, acc.: 50.00%] [G loss: 0.363613] [Loss change: -0.005, Loss increases: 0]\n",
      "200 [D loss: 0.788266, acc.: 50.00%] [G loss: 0.281872] [Loss change: 0.003, Loss increases: 1]\n",
      "300 [D loss: 0.864211, acc.: 50.00%] [G loss: 0.251239] [Loss change: 0.000, Loss increases: 0]\n",
      "400 [D loss: 0.920993, acc.: 49.61%] [G loss: 0.222996] [Loss change: -0.016, Loss increases: 2]\n",
      "500 [D loss: 0.895523, acc.: 50.00%] [G loss: 0.233865] [Loss change: -0.006, Loss increases: 2]\n",
      "600 [D loss: 0.872145, acc.: 50.00%] [G loss: 0.223788] [Loss change: 0.002, Loss increases: 0]\n",
      "700 [D loss: 0.914726, acc.: 50.00%] [G loss: 0.224767] [Loss change: 0.000, Loss increases: 0]\n",
      "800 [D loss: 0.900411, acc.: 50.00%] [G loss: 0.223209] [Loss change: -0.006, Loss increases: 1]\n",
      "900 [D loss: 0.888718, acc.: 50.20%] [G loss: 0.229103] [Loss change: 0.008, Loss increases: 0]\n",
      "1000 [D loss: 0.900703, acc.: 50.00%] [G loss: 0.226663] [Loss change: 0.002, Loss increases: 0]\n",
      "1100 [D loss: 0.883481, acc.: 50.20%] [G loss: 0.237712] [Loss change: -0.006, Loss increases: 2]\n",
      "1200 [D loss: 0.853766, acc.: 50.78%] [G loss: 0.240668] [Loss change: -0.003, Loss increases: 0]\n",
      "1300 [D loss: 0.827863, acc.: 50.00%] [G loss: 0.253103] [Loss change: 0.013, Loss increases: 0]\n",
      "1400 [D loss: 0.807776, acc.: 50.00%] [G loss: 0.251482] [Loss change: 0.004, Loss increases: 0]\n",
      "1500 [D loss: 0.796358, acc.: 49.80%] [G loss: 0.260542] [Loss change: 0.008, Loss increases: 0]\n",
      "1600 [D loss: 0.810544, acc.: 49.41%] [G loss: 0.263508] [Loss change: 0.001, Loss increases: 0]\n",
      "1700 [D loss: 0.784458, acc.: 49.41%] [G loss: 0.278623] [Loss change: 0.005, Loss increases: 0]\n",
      "1800 [D loss: 0.762590, acc.: 50.20%] [G loss: 0.282398] [Loss change: -0.003, Loss increases: 0]\n",
      "1900 [D loss: 0.752127, acc.: 50.39%] [G loss: 0.290000] [Loss change: 0.004, Loss increases: 0]\n",
      "2000 [D loss: 0.721849, acc.: 50.20%] [G loss: 0.291558] [Loss change: -0.004, Loss increases: 0]\n",
      "2100 [D loss: 0.729556, acc.: 50.39%] [G loss: 0.305438] [Loss change: 0.006, Loss increases: 0]\n",
      "2200 [D loss: 0.728700, acc.: 51.37%] [G loss: 0.316342] [Loss change: 0.005, Loss increases: 1]\n",
      "2300 [D loss: 0.726987, acc.: 51.17%] [G loss: 0.331167] [Loss change: 0.008, Loss increases: 0]\n",
      "2400 [D loss: 0.735583, acc.: 51.76%] [G loss: 0.330753] [Loss change: 0.002, Loss increases: 2]\n",
      "2500 [D loss: 0.706783, acc.: 52.34%] [G loss: 0.338228] [Loss change: 0.010, Loss increases: 3]\n",
      "2600 [D loss: 0.697391, acc.: 53.32%] [G loss: 0.345005] [Loss change: -0.001, Loss increases: 0]\n",
      "2700 [D loss: 0.704448, acc.: 53.32%] [G loss: 0.351022] [Loss change: 0.007, Loss increases: 0]\n",
      "2800 [D loss: 0.715868, acc.: 52.93%] [G loss: 0.349999] [Loss change: -0.005, Loss increases: 2]\n",
      "2900 [D loss: 0.715931, acc.: 55.86%] [G loss: 0.358166] [Loss change: 0.005, Loss increases: 1]\n",
      "Stoping on iteration:  2958\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 15. 13.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 15. 13.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0\n",
      " 0 1 1 1 1 1 1 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1]\n",
      "Attack type: normal.     number predicted:  43\n",
      "Attack type: smurf.     number predicted:  213\n",
      "\n",
      "[[  0   0]\n",
      " [ 43 213]]\n",
      "total: 256\n",
      "accuracy: 0.83203125\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25 25\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25 25\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 1.372180, acc.: 44.92%] [G loss: 0.763782] [Loss change: 0.764, Loss increases: 0]\n",
      "100 [D loss: 0.545336, acc.: 50.20%] [G loss: 0.435527] [Loss change: -0.003, Loss increases: 1]\n",
      "200 [D loss: 0.767273, acc.: 50.39%] [G loss: 0.293961] [Loss change: -0.002, Loss increases: 1]\n",
      "300 [D loss: 0.844100, acc.: 50.59%] [G loss: 0.250611] [Loss change: 0.004, Loss increases: 0]\n",
      "400 [D loss: 0.894448, acc.: 50.59%] [G loss: 0.240088] [Loss change: 0.007, Loss increases: 0]\n",
      "500 [D loss: 0.883534, acc.: 51.17%] [G loss: 0.231181] [Loss change: 0.000, Loss increases: 0]\n",
      "600 [D loss: 0.889702, acc.: 50.59%] [G loss: 0.223325] [Loss change: -0.003, Loss increases: 1]\n",
      "700 [D loss: 0.894868, acc.: 50.00%] [G loss: 0.213038] [Loss change: -0.006, Loss increases: 1]\n",
      "800 [D loss: 0.886901, acc.: 49.80%] [G loss: 0.223192] [Loss change: -0.006, Loss increases: 1]\n",
      "900 [D loss: 0.855329, acc.: 50.00%] [G loss: 0.229773] [Loss change: 0.005, Loss increases: 0]\n",
      "1000 [D loss: 0.846405, acc.: 49.80%] [G loss: 0.241018] [Loss change: 0.005, Loss increases: 0]\n",
      "1100 [D loss: 0.822465, acc.: 50.00%] [G loss: 0.249550] [Loss change: 0.001, Loss increases: 3]\n",
      "1200 [D loss: 0.822635, acc.: 49.80%] [G loss: 0.251290] [Loss change: -0.000, Loss increases: 0]\n",
      "1300 [D loss: 0.833033, acc.: 50.00%] [G loss: 0.249967] [Loss change: -0.021, Loss increases: 1]\n",
      "1400 [D loss: 0.788048, acc.: 50.00%] [G loss: 0.269591] [Loss change: -0.001, Loss increases: 0]\n",
      "1500 [D loss: 0.775160, acc.: 49.80%] [G loss: 0.276308] [Loss change: -0.014, Loss increases: 2]\n",
      "1600 [D loss: 0.758525, acc.: 49.80%] [G loss: 0.292918] [Loss change: 0.005, Loss increases: 0]\n",
      "1700 [D loss: 0.740346, acc.: 49.61%] [G loss: 0.296976] [Loss change: -0.000, Loss increases: 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1800 [D loss: 0.746961, acc.: 50.00%] [G loss: 0.299782] [Loss change: -0.009, Loss increases: 2]\n",
      "1900 [D loss: 0.740326, acc.: 50.00%] [G loss: 0.318409] [Loss change: -0.007, Loss increases: 2]\n",
      "2000 [D loss: 0.703898, acc.: 50.00%] [G loss: 0.331572] [Loss change: -0.001, Loss increases: 0]\n",
      "2100 [D loss: 0.682882, acc.: 49.41%] [G loss: 0.353279] [Loss change: 0.010, Loss increases: 0]\n",
      "2200 [D loss: 0.671015, acc.: 50.00%] [G loss: 0.362897] [Loss change: -0.005, Loss increases: 1]\n",
      "2300 [D loss: 0.686701, acc.: 49.02%] [G loss: 0.372910] [Loss change: -0.012, Loss increases: 1]\n",
      "2400 [D loss: 0.664711, acc.: 49.22%] [G loss: 0.400424] [Loss change: 0.008, Loss increases: 1]\n",
      "2500 [D loss: 0.629667, acc.: 49.22%] [G loss: 0.414556] [Loss change: 0.003, Loss increases: 0]\n",
      "2600 [D loss: 0.595640, acc.: 50.00%] [G loss: 0.433688] [Loss change: 0.009, Loss increases: 0]\n",
      "2700 [D loss: 0.604124, acc.: 49.41%] [G loss: 0.446339] [Loss change: -0.010, Loss increases: 1]\n",
      "2800 [D loss: 0.575466, acc.: 49.80%] [G loss: 0.458533] [Loss change: -0.014, Loss increases: 1]\n",
      "2900 [D loss: 0.558578, acc.: 50.39%] [G loss: 0.492519] [Loss change: 0.009, Loss increases: 0]\n",
      "3000 [D loss: 0.543252, acc.: 51.95%] [G loss: 0.515958] [Loss change: 0.007, Loss increases: 1]\n",
      "3100 [D loss: 0.533420, acc.: 52.34%] [G loss: 0.527329] [Loss change: -0.001, Loss increases: 1]\n",
      "3200 [D loss: 0.526056, acc.: 51.37%] [G loss: 0.534767] [Loss change: -0.019, Loss increases: 2]\n",
      "3300 [D loss: 0.501925, acc.: 56.25%] [G loss: 0.548710] [Loss change: 0.001, Loss increases: 0]\n",
      "3400 [D loss: 0.521977, acc.: 55.66%] [G loss: 0.558243] [Loss change: -0.002, Loss increases: 1]\n",
      "3500 [D loss: 0.528575, acc.: 54.69%] [G loss: 0.561069] [Loss change: 0.008, Loss increases: 0]\n",
      "3600 [D loss: 0.517087, acc.: 56.64%] [G loss: 0.576299] [Loss change: 0.007, Loss increases: 0]\n",
      "3700 [D loss: 0.513016, acc.: 58.01%] [G loss: 0.583024] [Loss change: 0.003, Loss increases: 2]\n",
      "3800 [D loss: 0.508369, acc.: 58.40%] [G loss: 0.581422] [Loss change: 0.003, Loss increases: 1]\n",
      "3900 [D loss: 0.521798, acc.: 58.59%] [G loss: 0.597792] [Loss change: -0.018, Loss increases: 0]\n",
      "4000 [D loss: 0.483945, acc.: 63.67%] [G loss: 0.615695] [Loss change: 0.005, Loss increases: 0]\n",
      "4100 [D loss: 0.489318, acc.: 64.06%] [G loss: 0.618117] [Loss change: -0.007, Loss increases: 0]\n",
      "4200 [D loss: 0.505722, acc.: 65.04%] [G loss: 0.638693] [Loss change: 0.010, Loss increases: 0]\n",
      "4300 [D loss: 0.485294, acc.: 67.38%] [G loss: 0.656506] [Loss change: -0.020, Loss increases: 2]\n",
      "Stoping on iteration:  4357\n",
      "Generated Smurf attacks: \n",
      "[[ 4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0. 36.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  5.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  3.  0.  0.  0.  0.  1.  0.  2.  0.  0.  2.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 0 0 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 1 1 0 0 1 0 0 1 0 1 0 1 0 0 1 0 1 1 0 0 0 1\n",
      " 1 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 0 1 0 1 1 0 1 0 1\n",
      " 0 0 1 1 1 0 1 0 0 1 1 1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 1 1 1 1 0 0 1 0 0 0 0\n",
      " 1 0 1 1 1 1 0 1 1 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 0\n",
      " 0 1 1 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 0 1 0 0 1 0 1 1 1 0 1 0 0 0 1 0 0 1 0 0 1 1 1 1 1 0]\n",
      "Attack type: normal.     number predicted:  103\n",
      "Attack type: smurf.     number predicted:  153\n",
      "\n",
      "[[  0   0]\n",
      " [103 153]]\n",
      "total: 256\n",
      "accuracy: 0.59765625\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 29\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.383594, acc.: 51.17%] [G loss: 0.575862] [Loss change: 0.576, Loss increases: 0]\n",
      "100 [D loss: 0.522416, acc.: 50.00%] [G loss: 0.457955] [Loss change: -0.001, Loss increases: 0]\n",
      "200 [D loss: 0.559052, acc.: 50.00%] [G loss: 0.430666] [Loss change: 0.007, Loss increases: 0]\n",
      "300 [D loss: 0.562444, acc.: 50.20%] [G loss: 0.420551] [Loss change: -0.005, Loss increases: 1]\n",
      "400 [D loss: 0.567795, acc.: 50.20%] [G loss: 0.429614] [Loss change: 0.001, Loss increases: 1]\n",
      "500 [D loss: 0.560714, acc.: 50.00%] [G loss: 0.431672] [Loss change: -0.000, Loss increases: 1]\n",
      "600 [D loss: 0.549999, acc.: 50.00%] [G loss: 0.438522] [Loss change: 0.005, Loss increases: 0]\n",
      "700 [D loss: 0.555377, acc.: 51.17%] [G loss: 0.435148] [Loss change: -0.001, Loss increases: 2]\n",
      "800 [D loss: 0.551341, acc.: 50.78%] [G loss: 0.447113] [Loss change: 0.002, Loss increases: 0]\n",
      "900 [D loss: 0.536251, acc.: 50.59%] [G loss: 0.449995] [Loss change: 0.006, Loss increases: 0]\n",
      "1000 [D loss: 0.514962, acc.: 52.93%] [G loss: 0.463149] [Loss change: 0.011, Loss increases: 0]\n",
      "1100 [D loss: 0.525082, acc.: 51.76%] [G loss: 0.465288] [Loss change: -0.001, Loss increases: 0]\n",
      "1200 [D loss: 0.500573, acc.: 53.71%] [G loss: 0.487116] [Loss change: 0.005, Loss increases: 0]\n",
      "Stoping on iteration:  1222\n",
      "Generated Smurf attacks: \n",
      "[[0. 6. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 2. 7. 5.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 5.]\n",
      " [1. 0. 0. 0. 0. 0. 7. 1. 0. 0. 2. 4. 4. 0. 0. 0. 3. 1. 0. 1. 0. 0. 0. 0.\n",
      "  0. 0. 5. 2. 0. 0. 1. 0. 5. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Attack type: normal.     number predicted:  256\n",
      "\n",
      "[[  0   0]\n",
      " [256   0]]\n",
      "total: 256\n",
      "accuracy: 0.0\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33 33\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33 33\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.529207, acc.: 76.56%] [G loss: 0.743703] [Loss change: 0.744, Loss increases: 0]\n",
      "100 [D loss: 0.631052, acc.: 50.20%] [G loss: 0.368387] [Loss change: 0.001, Loss increases: 1]\n",
      "200 [D loss: 0.680850, acc.: 50.20%] [G loss: 0.335298] [Loss change: 0.013, Loss increases: 0]\n",
      "300 [D loss: 0.699827, acc.: 50.59%] [G loss: 0.334904] [Loss change: 0.013, Loss increases: 1]\n",
      "400 [D loss: 0.750669, acc.: 49.80%] [G loss: 0.323502] [Loss change: 0.012, Loss increases: 0]\n",
      "500 [D loss: 0.726597, acc.: 50.00%] [G loss: 0.327026] [Loss change: 0.005, Loss increases: 3]\n",
      "600 [D loss: 0.725679, acc.: 50.20%] [G loss: 0.317129] [Loss change: 0.002, Loss increases: 0]\n",
      "700 [D loss: 0.720952, acc.: 50.00%] [G loss: 0.317186] [Loss change: 0.005, Loss increases: 0]\n",
      "800 [D loss: 0.758254, acc.: 50.39%] [G loss: 0.321366] [Loss change: 0.003, Loss increases: 0]\n",
      "Stoping on iteration:  834\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0. 12.  0.  0.  0.  0.  2.  0.  0.  0.  0.\n",
      "   0.  0.  0.  3. 20.  0.  1.  0.  0.  0.  2.  1. 17.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  2.  0.  0.  0.  0.\n",
      "   0.  0.  0.  3. 24. 18.  0.  0.  0.  0.  1.  2.  2.  0.  2.  0.  0.  0.\n",
      "   0.  1.  0.  1.  0.]]\n",
      "[1 1 1 0 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 0 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 1 1 0 1 1 1 1 0 1 0 1 0 0 1\n",
      " 1 0 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1\n",
      " 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 0 1 0 1 1 1 1 0 1 0 1 1 1 1 1 1 0 1 0 0\n",
      " 0 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1]\n",
      "Attack type: normal.     number predicted:  64\n",
      "Attack type: smurf.     number predicted:  192\n",
      "\n",
      "[[  0   0]\n",
      " [ 64 192]]\n",
      "total: 256\n",
      "accuracy: 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 29\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 29\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.412177, acc.: 50.00%] [G loss: 0.355540] [Loss change: 0.356, Loss increases: 0]\n",
      "100 [D loss: 1.024772, acc.: 50.00%] [G loss: 0.175924] [Loss change: -0.007, Loss increases: 1]\n",
      "200 [D loss: 1.018396, acc.: 50.00%] [G loss: 0.170692] [Loss change: -0.003, Loss increases: 0]\n",
      "300 [D loss: 1.071105, acc.: 50.00%] [G loss: 0.165448] [Loss change: 0.001, Loss increases: 0]\n",
      "400 [D loss: 1.052665, acc.: 49.80%] [G loss: 0.166338] [Loss change: -0.000, Loss increases: 0]\n",
      "500 [D loss: 1.038666, acc.: 49.61%] [G loss: 0.170035] [Loss change: 0.001, Loss increases: 0]\n",
      "600 [D loss: 1.055449, acc.: 49.22%] [G loss: 0.166851] [Loss change: -0.004, Loss increases: 0]\n",
      "700 [D loss: 1.020982, acc.: 50.00%] [G loss: 0.166477] [Loss change: 0.002, Loss increases: 0]\n",
      "800 [D loss: 1.010128, acc.: 49.61%] [G loss: 0.170273] [Loss change: -0.006, Loss increases: 1]\n",
      "900 [D loss: 0.981902, acc.: 50.00%] [G loss: 0.176448] [Loss change: 0.003, Loss increases: 1]\n",
      "1000 [D loss: 1.014879, acc.: 50.00%] [G loss: 0.180111] [Loss change: 0.009, Loss increases: 0]\n",
      "1100 [D loss: 1.068684, acc.: 49.61%] [G loss: 0.171906] [Loss change: -0.002, Loss increases: 1]\n",
      "1200 [D loss: 1.119142, acc.: 49.41%] [G loss: 0.168700] [Loss change: 0.002, Loss increases: 0]\n",
      "1300 [D loss: 1.090551, acc.: 49.41%] [G loss: 0.178144] [Loss change: 0.007, Loss increases: 4]\n",
      "1400 [D loss: 1.131800, acc.: 49.41%] [G loss: 0.167874] [Loss change: -0.003, Loss increases: 2]\n",
      "1500 [D loss: 1.132805, acc.: 49.61%] [G loss: 0.169814] [Loss change: 0.011, Loss increases: 0]\n",
      "1600 [D loss: 1.203880, acc.: 49.61%] [G loss: 0.164098] [Loss change: 0.006, Loss increases: 2]\n",
      "1700 [D loss: 1.230268, acc.: 49.80%] [G loss: 0.157610] [Loss change: 0.007, Loss increases: 0]\n",
      "1800 [D loss: 1.258567, acc.: 49.80%] [G loss: 0.140868] [Loss change: -0.002, Loss increases: 0]\n",
      "1900 [D loss: 1.208997, acc.: 49.80%] [G loss: 0.149258] [Loss change: 0.009, Loss increases: 2]\n",
      "2000 [D loss: 1.229348, acc.: 49.80%] [G loss: 0.134472] [Loss change: 0.002, Loss increases: 1]\n",
      "2100 [D loss: 1.242269, acc.: 49.22%] [G loss: 0.138716] [Loss change: 0.004, Loss increases: 1]\n",
      "2200 [D loss: 1.196487, acc.: 49.61%] [G loss: 0.143968] [Loss change: 0.004, Loss increases: 0]\n",
      "2300 [D loss: 1.161938, acc.: 49.80%] [G loss: 0.146951] [Loss change: 0.001, Loss increases: 0]\n",
      "2400 [D loss: 1.136905, acc.: 49.22%] [G loss: 0.151258] [Loss change: -0.004, Loss increases: 2]\n",
      "2500 [D loss: 1.140683, acc.: 49.80%] [G loss: 0.157070] [Loss change: -0.007, Loss increases: 0]\n",
      "2600 [D loss: 1.103632, acc.: 49.80%] [G loss: 0.173626] [Loss change: 0.013, Loss increases: 0]\n",
      "2700 [D loss: 1.174147, acc.: 50.00%] [G loss: 0.177290] [Loss change: 0.009, Loss increases: 0]\n",
      "2800 [D loss: 1.182409, acc.: 49.80%] [G loss: 0.179917] [Loss change: 0.010, Loss increases: 0]\n",
      "2900 [D loss: 1.225154, acc.: 49.41%] [G loss: 0.171200] [Loss change: -0.006, Loss increases: 1]\n",
      "Stoping on iteration:  2944\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0. 16.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 59. 39.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  8.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 54. 44.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 0 1 0 1 1 0 0 1 1 1 1 1 1 1 1 0 1 0 0 1 1 0 1 1 1 0 1 0 1 1 0 1 1 1 1\n",
      " 0 1 1 0 1 1 1 1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 0 1 0 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 0\n",
      " 1 1 0 1 1 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1\n",
      " 1 0 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1]\n",
      "Attack type: normal.     number predicted:  64\n",
      "Attack type: smurf.     number predicted:  192\n",
      "\n",
      "[[  0   0]\n",
      " [ 64 192]]\n",
      "total: 256\n",
      "accuracy: 0.75\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 29 29\n",
      "   0  0  0  0  0  0  0  1  1  1  0  1  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 27 27\n",
      "   0  0  0  0  0  0  0  1  1  1  0  1  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.569964, acc.: 72.85%] [G loss: 0.674174] [Loss change: 0.674, Loss increases: 0]\n",
      "100 [D loss: 0.719769, acc.: 50.00%] [G loss: 0.339522] [Loss change: -0.002, Loss increases: 0]\n",
      "200 [D loss: 0.808884, acc.: 49.80%] [G loss: 0.288316] [Loss change: 0.013, Loss increases: 0]\n",
      "300 [D loss: 0.870672, acc.: 50.78%] [G loss: 0.247173] [Loss change: -0.015, Loss increases: 1]\n",
      "400 [D loss: 0.868190, acc.: 50.39%] [G loss: 0.231757] [Loss change: 0.014, Loss increases: 0]\n",
      "500 [D loss: 0.889682, acc.: 50.00%] [G loss: 0.207482] [Loss change: -0.007, Loss increases: 1]\n",
      "600 [D loss: 0.883347, acc.: 50.00%] [G loss: 0.223415] [Loss change: 0.005, Loss increases: 0]\n",
      "700 [D loss: 0.870142, acc.: 50.00%] [G loss: 0.234296] [Loss change: 0.007, Loss increases: 0]\n",
      "800 [D loss: 0.857963, acc.: 49.80%] [G loss: 0.235492] [Loss change: -0.000, Loss increases: 1]\n",
      "900 [D loss: 0.849208, acc.: 49.61%] [G loss: 0.239458] [Loss change: 0.001, Loss increases: 0]\n",
      "1000 [D loss: 0.821889, acc.: 50.00%] [G loss: 0.254611] [Loss change: 0.005, Loss increases: 0]\n",
      "1100 [D loss: 0.816401, acc.: 49.80%] [G loss: 0.268599] [Loss change: 0.010, Loss increases: 1]\n",
      "1200 [D loss: 0.803971, acc.: 49.02%] [G loss: 0.272960] [Loss change: 0.006, Loss increases: 0]\n",
      "1300 [D loss: 0.816146, acc.: 49.61%] [G loss: 0.280951] [Loss change: 0.001, Loss increases: 1]\n",
      "Stoping on iteration:  1361\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  2.  0. 48.  0.  0.  0.  0.  0.  0.  0.  7.  0.  0.  0.  0.\n",
      "   0.  0.  0.  9.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  3.  0. 52.  0.  0.  0.  0.  0.  0.  0.  8.  0.  0.  0.  0.\n",
      "   0.  0.  0.  9.  0.]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Attack type: smurf.     number predicted:  256\n",
      "\n",
      "[[256]]\n",
      "total: 256\n",
      "accuracy: 1.0\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.514435, acc.: 78.12%] [G loss: 0.786305] [Loss change: 0.786, Loss increases: 0]\n",
      "100 [D loss: 0.708545, acc.: 49.80%] [G loss: 0.350031] [Loss change: 0.010, Loss increases: 0]\n",
      "200 [D loss: 0.784657, acc.: 50.59%] [G loss: 0.332173] [Loss change: -0.014, Loss increases: 1]\n",
      "300 [D loss: 0.846774, acc.: 50.00%] [G loss: 0.325037] [Loss change: -0.017, Loss increases: 1]\n",
      "400 [D loss: 0.784090, acc.: 49.80%] [G loss: 0.337628] [Loss change: 0.002, Loss increases: 0]\n",
      "500 [D loss: 0.762089, acc.: 50.00%] [G loss: 0.326204] [Loss change: 0.005, Loss increases: 0]\n",
      "600 [D loss: 0.809267, acc.: 50.20%] [G loss: 0.323925] [Loss change: -0.001, Loss increases: 0]\n",
      "700 [D loss: 0.811882, acc.: 51.37%] [G loss: 0.317567] [Loss change: -0.003, Loss increases: 0]\n",
      "800 [D loss: 0.868739, acc.: 50.98%] [G loss: 0.309113] [Loss change: -0.020, Loss increases: 1]\n",
      "900 [D loss: 0.890220, acc.: 50.20%] [G loss: 0.298426] [Loss change: -0.007, Loss increases: 0]\n",
      "1000 [D loss: 0.898072, acc.: 52.15%] [G loss: 0.286549] [Loss change: -0.035, Loss increases: 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1100 [D loss: 0.966074, acc.: 51.95%] [G loss: 0.285716] [Loss change: -0.026, Loss increases: 1]\n",
      "1200 [D loss: 0.989661, acc.: 52.15%] [G loss: 0.274708] [Loss change: -0.014, Loss increases: 2]\n",
      "1300 [D loss: 0.973209, acc.: 51.76%] [G loss: 0.258805] [Loss change: -0.011, Loss increases: 1]\n",
      "1400 [D loss: 0.964913, acc.: 50.20%] [G loss: 0.270557] [Loss change: 0.004, Loss increases: 2]\n",
      "1500 [D loss: 0.992721, acc.: 50.78%] [G loss: 0.277195] [Loss change: 0.011, Loss increases: 1]\n",
      "1600 [D loss: 1.054746, acc.: 50.59%] [G loss: 0.257157] [Loss change: -0.003, Loss increases: 0]\n",
      "1700 [D loss: 1.071178, acc.: 50.20%] [G loss: 0.257933] [Loss change: 0.022, Loss increases: 0]\n",
      "1800 [D loss: 1.076211, acc.: 51.95%] [G loss: 0.236343] [Loss change: -0.013, Loss increases: 1]\n",
      "1900 [D loss: 1.117893, acc.: 49.61%] [G loss: 0.224511] [Loss change: -0.016, Loss increases: 1]\n",
      "2000 [D loss: 1.144363, acc.: 50.59%] [G loss: 0.227577] [Loss change: -0.003, Loss increases: 0]\n",
      "2100 [D loss: 1.123589, acc.: 50.78%] [G loss: 0.236535] [Loss change: 0.004, Loss increases: 0]\n",
      "2200 [D loss: 1.124984, acc.: 52.34%] [G loss: 0.249274] [Loss change: 0.027, Loss increases: 0]\n",
      "2300 [D loss: 1.176472, acc.: 52.15%] [G loss: 0.248930] [Loss change: 0.006, Loss increases: 1]\n",
      "2400 [D loss: 1.191645, acc.: 50.98%] [G loss: 0.240769] [Loss change: -0.000, Loss increases: 1]\n",
      "2500 [D loss: 1.224970, acc.: 51.76%] [G loss: 0.226713] [Loss change: -0.005, Loss increases: 1]\n",
      "2600 [D loss: 1.192521, acc.: 53.12%] [G loss: 0.238475] [Loss change: 0.016, Loss increases: 0]\n",
      "2700 [D loss: 1.248184, acc.: 50.98%] [G loss: 0.227961] [Loss change: 0.013, Loss increases: 0]\n",
      "2800 [D loss: 1.282913, acc.: 50.98%] [G loss: 0.236877] [Loss change: 0.004, Loss increases: 0]\n",
      "2900 [D loss: 1.246633, acc.: 53.32%] [G loss: 0.231146] [Loss change: -0.007, Loss increases: 1]\n",
      "Stoping on iteration:  2921\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0. 15.  0.  0.  1.  0.  0. 21. 18.  0. 14.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0. 15.  0.  0.  0.  4.  0.  0.  0.  0.  0.  0. 17.  0.  0.  0.\n",
      "   0.  0.  0.  6.  8.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 54. 56.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 1 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 1 1\n",
      " 1 1 1 0 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 0 0 0 1 1\n",
      " 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 1 1 1 0 1 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1]\n",
      "Attack type: normal.     number predicted:  47\n",
      "Attack type: smurf.     number predicted:  209\n",
      "\n",
      "[[  0   0]\n",
      " [ 47 209]]\n",
      "total: 256\n",
      "accuracy: 0.81640625\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 3.745091, acc.: 48.83%] [G loss: 1.152435] [Loss change: 1.152, Loss increases: 0]\n",
      "100 [D loss: 0.655196, acc.: 50.98%] [G loss: 0.361944] [Loss change: -0.004, Loss increases: 0]\n",
      "200 [D loss: 0.941973, acc.: 50.00%] [G loss: 0.221323] [Loss change: -0.021, Loss increases: 0]\n",
      "300 [D loss: 0.991695, acc.: 50.00%] [G loss: 0.209165] [Loss change: 0.000, Loss increases: 1]\n",
      "400 [D loss: 1.023286, acc.: 50.98%] [G loss: 0.215067] [Loss change: 0.024, Loss increases: 0]\n",
      "500 [D loss: 1.040493, acc.: 50.20%] [G loss: 0.186953] [Loss change: 0.003, Loss increases: 0]\n",
      "600 [D loss: 1.006029, acc.: 50.00%] [G loss: 0.190309] [Loss change: 0.003, Loss increases: 1]\n",
      "700 [D loss: 1.010974, acc.: 49.80%] [G loss: 0.173895] [Loss change: -0.001, Loss increases: 0]\n",
      "800 [D loss: 1.019836, acc.: 49.80%] [G loss: 0.175420] [Loss change: -0.008, Loss increases: 0]\n",
      "900 [D loss: 0.983656, acc.: 49.61%] [G loss: 0.187401] [Loss change: 0.003, Loss increases: 0]\n",
      "1000 [D loss: 0.978489, acc.: 49.80%] [G loss: 0.195294] [Loss change: 0.002, Loss increases: 1]\n",
      "1100 [D loss: 0.963401, acc.: 50.00%] [G loss: 0.199263] [Loss change: -0.001, Loss increases: 0]\n",
      "1200 [D loss: 0.934136, acc.: 49.22%] [G loss: 0.208525] [Loss change: 0.006, Loss increases: 0]\n",
      "1300 [D loss: 0.937788, acc.: 49.41%] [G loss: 0.210772] [Loss change: 0.000, Loss increases: 1]\n",
      "1400 [D loss: 0.924200, acc.: 49.41%] [G loss: 0.221551] [Loss change: 0.015, Loss increases: 0]\n",
      "Stoping on iteration:  1422\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 36.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0. 61.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Attack type: smurf.     number predicted:  256\n",
      "\n",
      "[[256]]\n",
      "total: 256\n",
      "accuracy: 1.0\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 16 16\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  6\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 2.027737, acc.: 28.91%] [G loss: 0.796028] [Loss change: 0.796, Loss increases: 0]\n",
      "100 [D loss: 0.805661, acc.: 50.00%] [G loss: 0.294777] [Loss change: -0.003, Loss increases: 1]\n",
      "200 [D loss: 1.037540, acc.: 50.78%] [G loss: 0.218345] [Loss change: 0.001, Loss increases: 1]\n",
      "300 [D loss: 0.997077, acc.: 50.00%] [G loss: 0.217995] [Loss change: 0.008, Loss increases: 2]\n",
      "400 [D loss: 1.016063, acc.: 50.20%] [G loss: 0.192955] [Loss change: 0.007, Loss increases: 0]\n",
      "500 [D loss: 1.046968, acc.: 50.00%] [G loss: 0.181583] [Loss change: 0.009, Loss increases: 0]\n",
      "600 [D loss: 1.083694, acc.: 50.00%] [G loss: 0.155998] [Loss change: -0.012, Loss increases: 1]\n",
      "700 [D loss: 1.113459, acc.: 49.80%] [G loss: 0.146628] [Loss change: -0.007, Loss increases: 2]\n",
      "800 [D loss: 1.093351, acc.: 49.41%] [G loss: 0.158987] [Loss change: 0.002, Loss increases: 2]\n",
      "900 [D loss: 1.061752, acc.: 50.00%] [G loss: 0.163217] [Loss change: -0.000, Loss increases: 2]\n",
      "1000 [D loss: 1.031652, acc.: 50.00%] [G loss: 0.168356] [Loss change: -0.005, Loss increases: 0]\n",
      "1100 [D loss: 1.022775, acc.: 49.41%] [G loss: 0.178250] [Loss change: -0.003, Loss increases: 0]\n",
      "1200 [D loss: 1.039289, acc.: 49.22%] [G loss: 0.189293] [Loss change: 0.007, Loss increases: 0]\n",
      "1300 [D loss: 0.978951, acc.: 50.00%] [G loss: 0.201703] [Loss change: 0.006, Loss increases: 0]\n",
      "1400 [D loss: 1.012577, acc.: 49.61%] [G loss: 0.193833] [Loss change: -0.008, Loss increases: 0]\n",
      "1500 [D loss: 0.972843, acc.: 49.80%] [G loss: 0.211298] [Loss change: -0.006, Loss increases: 4]\n",
      "1600 [D loss: 0.954252, acc.: 49.22%] [G loss: 0.221137] [Loss change: -0.002, Loss increases: 0]\n",
      "1700 [D loss: 0.929526, acc.: 49.80%] [G loss: 0.229331] [Loss change: -0.006, Loss increases: 1]\n",
      "1800 [D loss: 0.922695, acc.: 49.61%] [G loss: 0.234913] [Loss change: -0.006, Loss increases: 2]\n",
      "1900 [D loss: 0.883519, acc.: 49.41%] [G loss: 0.260772] [Loss change: 0.009, Loss increases: 0]\n",
      "2000 [D loss: 0.920319, acc.: 48.44%] [G loss: 0.254502] [Loss change: -0.012, Loss increases: 1]\n",
      "2100 [D loss: 0.860256, acc.: 49.80%] [G loss: 0.277111] [Loss change: 0.001, Loss increases: 2]\n",
      "2200 [D loss: 0.869067, acc.: 49.22%] [G loss: 0.286488] [Loss change: 0.017, Loss increases: 0]\n",
      "2300 [D loss: 0.895369, acc.: 49.22%] [G loss: 0.285189] [Loss change: -0.001, Loss increases: 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 [D loss: 0.856407, acc.: 49.22%] [G loss: 0.307188] [Loss change: 0.001, Loss increases: 0]\n",
      "2500 [D loss: 0.868314, acc.: 48.63%] [G loss: 0.315089] [Loss change: 0.006, Loss increases: 0]\n",
      "2600 [D loss: 0.870165, acc.: 48.05%] [G loss: 0.323533] [Loss change: -0.005, Loss increases: 0]\n",
      "2700 [D loss: 0.861470, acc.: 49.41%] [G loss: 0.341017] [Loss change: -0.005, Loss increases: 3]\n",
      "2800 [D loss: 0.863443, acc.: 49.02%] [G loss: 0.348392] [Loss change: -0.010, Loss increases: 2]\n",
      "2900 [D loss: 0.832983, acc.: 49.02%] [G loss: 0.366706] [Loss change: 0.011, Loss increases: 0]\n",
      "3000 [D loss: 0.791599, acc.: 49.80%] [G loss: 0.402550] [Loss change: 0.032, Loss increases: 0]\n",
      "3100 [D loss: 0.752916, acc.: 49.41%] [G loss: 0.412651] [Loss change: -0.001, Loss increases: 1]\n",
      "3200 [D loss: 0.774370, acc.: 49.02%] [G loss: 0.430347] [Loss change: 0.029, Loss increases: 0]\n",
      "3300 [D loss: 0.846478, acc.: 48.05%] [G loss: 0.433283] [Loss change: -0.001, Loss increases: 1]\n",
      "3400 [D loss: 0.829615, acc.: 48.83%] [G loss: 0.451785] [Loss change: 0.004, Loss increases: 0]\n",
      "3500 [D loss: 0.785738, acc.: 49.41%] [G loss: 0.466931] [Loss change: -0.002, Loss increases: 0]\n",
      "3600 [D loss: 0.837607, acc.: 49.41%] [G loss: 0.474470] [Loss change: -0.004, Loss increases: 0]\n",
      "3700 [D loss: 0.807806, acc.: 49.02%] [G loss: 0.512332] [Loss change: 0.019, Loss increases: 0]\n",
      "3800 [D loss: 0.831281, acc.: 49.41%] [G loss: 0.529761] [Loss change: -0.003, Loss increases: 1]\n",
      "3900 [D loss: 0.949106, acc.: 52.15%] [G loss: 0.515685] [Loss change: -0.016, Loss increases: 1]\n",
      "4000 [D loss: 1.035320, acc.: 66.21%] [G loss: 0.568270] [Loss change: 0.005, Loss increases: 0]\n",
      "4100 [D loss: 0.866810, acc.: 65.82%] [G loss: 0.542842] [Loss change: -0.011, Loss increases: 1]\n",
      "4200 [D loss: 0.845850, acc.: 61.72%] [G loss: 0.516610] [Loss change: -0.002, Loss increases: 1]\n",
      "4300 [D loss: 0.848264, acc.: 61.72%] [G loss: 0.502450] [Loss change: -0.006, Loss increases: 0]\n",
      "4400 [D loss: 0.848399, acc.: 60.55%] [G loss: 0.498143] [Loss change: 0.003, Loss increases: 0]\n",
      "4500 [D loss: 0.834309, acc.: 61.13%] [G loss: 0.496205] [Loss change: 0.009, Loss increases: 0]\n",
      "4600 [D loss: 0.841289, acc.: 61.33%] [G loss: 0.475316] [Loss change: -0.010, Loss increases: 0]\n",
      "4700 [D loss: 0.829876, acc.: 59.57%] [G loss: 0.483290] [Loss change: 0.003, Loss increases: 1]\n",
      "4800 [D loss: 0.854242, acc.: 57.42%] [G loss: 0.475734] [Loss change: 0.003, Loss increases: 0]\n",
      "Stoping on iteration:  4876\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  4.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 46. 46.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  4.  5.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1\n",
      " 1 0 1 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 1 1 0 1 1 1 1 0 1 0 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1\n",
      " 1 1 0 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1\n",
      " 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 0\n",
      " 0 1 1 1 1 1 0 1 0 1 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1]\n",
      "Attack type: normal.     number predicted:  45\n",
      "Attack type: smurf.     number predicted:  211\n",
      "\n",
      "[[  0   0]\n",
      " [ 45 211]]\n",
      "total: 256\n",
      "accuracy: 0.82421875\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33 33\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 33 33\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.689775, acc.: 66.60%] [G loss: 0.666940] [Loss change: 0.667, Loss increases: 0]\n",
      "100 [D loss: 0.774202, acc.: 50.00%] [G loss: 0.307624] [Loss change: -0.011, Loss increases: 1]\n",
      "200 [D loss: 0.888186, acc.: 50.00%] [G loss: 0.240979] [Loss change: 0.002, Loss increases: 0]\n",
      "300 [D loss: 0.913844, acc.: 50.00%] [G loss: 0.226523] [Loss change: -0.005, Loss increases: 0]\n",
      "Stoping on iteration:  378\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  3.  0.  5. 10.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  4.\n",
      "   0.  0.  3.  3.  0.]\n",
      " [ 0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  2.  6.  5.  0.  0.  0.  0.\n",
      "   4.  0.  0.  3.  7.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   1.  0.  0.  0.  1.]]\n",
      "[1 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 1 1 1 0 1 1 1\n",
      " 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 0 0\n",
      " 1 1 0 1 1 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 1 1 1 1 1 0 1 0 1 0\n",
      " 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 1 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 1 1 1 1 0 0\n",
      " 0 1 1 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 1 0 1 1 1 1 1 0 1 0 0 0 1 1 1 1\n",
      " 0 1 1 1 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 0 0 0 1]\n",
      "Attack type: normal.     number predicted:  141\n",
      "Attack type: smurf.     number predicted:  115\n",
      "\n",
      "[[  0   0]\n",
      " [141 115]]\n",
      "total: 256\n",
      "accuracy: 0.44921875\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.376316, acc.: 57.42%] [G loss: 0.549934] [Loss change: 0.550, Loss increases: 0]\n",
      "100 [D loss: 0.770228, acc.: 50.00%] [G loss: 0.304574] [Loss change: -0.008, Loss increases: 0]\n",
      "200 [D loss: 0.817470, acc.: 49.80%] [G loss: 0.267129] [Loss change: -0.013, Loss increases: 0]\n",
      "300 [D loss: 0.782095, acc.: 50.20%] [G loss: 0.274119] [Loss change: 0.002, Loss increases: 1]\n",
      "400 [D loss: 0.824936, acc.: 50.00%] [G loss: 0.277135] [Loss change: 0.010, Loss increases: 1]\n",
      "500 [D loss: 0.797017, acc.: 49.41%] [G loss: 0.274126] [Loss change: 0.005, Loss increases: 2]\n",
      "600 [D loss: 0.807632, acc.: 49.61%] [G loss: 0.274891] [Loss change: 0.001, Loss increases: 0]\n",
      "700 [D loss: 0.769872, acc.: 49.80%] [G loss: 0.288710] [Loss change: 0.007, Loss increases: 0]\n",
      "800 [D loss: 0.751492, acc.: 49.80%] [G loss: 0.289106] [Loss change: 0.010, Loss increases: 0]\n",
      "900 [D loss: 0.754641, acc.: 50.00%] [G loss: 0.292973] [Loss change: 0.004, Loss increases: 1]\n",
      "1000 [D loss: 0.766998, acc.: 49.80%] [G loss: 0.297110] [Loss change: 0.004, Loss increases: 0]\n",
      "1100 [D loss: 0.766458, acc.: 50.00%] [G loss: 0.302989] [Loss change: 0.007, Loss increases: 1]\n",
      "1200 [D loss: 0.750759, acc.: 50.00%] [G loss: 0.310483] [Loss change: 0.004, Loss increases: 1]\n",
      "1300 [D loss: 0.772174, acc.: 50.20%] [G loss: 0.312725] [Loss change: -0.004, Loss increases: 3]\n",
      "1400 [D loss: 0.783183, acc.: 50.39%] [G loss: 0.321703] [Loss change: 0.007, Loss increases: 0]\n",
      "1500 [D loss: 0.823046, acc.: 50.00%] [G loss: 0.314902] [Loss change: -0.006, Loss increases: 1]\n",
      "1600 [D loss: 0.861635, acc.: 50.39%] [G loss: 0.312945] [Loss change: -0.005, Loss increases: 0]\n",
      "1700 [D loss: 0.957097, acc.: 49.80%] [G loss: 0.272819] [Loss change: 0.003, Loss increases: 0]\n",
      "1800 [D loss: 1.004341, acc.: 49.22%] [G loss: 0.259727] [Loss change: 0.002, Loss increases: 1]\n",
      "1900 [D loss: 1.009728, acc.: 49.61%] [G loss: 0.263990] [Loss change: 0.011, Loss increases: 1]\n",
      "2000 [D loss: 1.019965, acc.: 50.98%] [G loss: 0.256950] [Loss change: 0.010, Loss increases: 0]\n",
      "2100 [D loss: 1.083589, acc.: 50.20%] [G loss: 0.254156] [Loss change: 0.006, Loss increases: 0]\n",
      "2200 [D loss: 1.117758, acc.: 49.61%] [G loss: 0.245153] [Loss change: -0.008, Loss increases: 3]\n",
      "2300 [D loss: 1.144786, acc.: 48.44%] [G loss: 0.248602] [Loss change: -0.000, Loss increases: 0]\n",
      "2400 [D loss: 1.180725, acc.: 48.83%] [G loss: 0.223421] [Loss change: -0.010, Loss increases: 0]\n",
      "2500 [D loss: 1.117764, acc.: 49.22%] [G loss: 0.243728] [Loss change: 0.001, Loss increases: 3]\n",
      "2600 [D loss: 1.109501, acc.: 48.63%] [G loss: 0.236400] [Loss change: -0.002, Loss increases: 0]\n",
      "2700 [D loss: 1.114868, acc.: 49.41%] [G loss: 0.243717] [Loss change: 0.011, Loss increases: 0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2800 [D loss: 1.145083, acc.: 49.02%] [G loss: 0.228794] [Loss change: -0.008, Loss increases: 0]\n",
      "2900 [D loss: 1.175910, acc.: 50.59%] [G loss: 0.249706] [Loss change: 0.004, Loss increases: 2]\n",
      "3000 [D loss: 1.170147, acc.: 51.17%] [G loss: 0.247164] [Loss change: 0.008, Loss increases: 2]\n",
      "3100 [D loss: 1.136567, acc.: 50.78%] [G loss: 0.244557] [Loss change: -0.001, Loss increases: 0]\n",
      "3200 [D loss: 1.150687, acc.: 49.61%] [G loss: 0.261465] [Loss change: 0.024, Loss increases: 0]\n",
      "3300 [D loss: 1.193667, acc.: 51.95%] [G loss: 0.249195] [Loss change: 0.001, Loss increases: 0]\n",
      "3400 [D loss: 1.177250, acc.: 51.95%] [G loss: 0.251563] [Loss change: -0.016, Loss increases: 1]\n",
      "3500 [D loss: 1.234760, acc.: 50.39%] [G loss: 0.259953] [Loss change: 0.003, Loss increases: 0]\n",
      "3600 [D loss: 1.207679, acc.: 50.00%] [G loss: 0.244876] [Loss change: -0.013, Loss increases: 2]\n",
      "3700 [D loss: 1.179514, acc.: 52.73%] [G loss: 0.257993] [Loss change: -0.000, Loss increases: 1]\n",
      "3800 [D loss: 1.155360, acc.: 56.05%] [G loss: 0.276973] [Loss change: 0.004, Loss increases: 1]\n",
      "3900 [D loss: 1.185788, acc.: 52.34%] [G loss: 0.256147] [Loss change: -0.018, Loss increases: 2]\n",
      "Stoping on iteration:  3967\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 55. 54.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 55. 54.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1\n",
      " 1 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0\n",
      " 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 0 1 0]\n",
      "Attack type: normal.     number predicted:  33\n",
      "Attack type: smurf.     number predicted:  223\n",
      "\n",
      "[[  0   0]\n",
      " [ 33 223]]\n",
      "total: 256\n",
      "accuracy: 0.87109375\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 28 28\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.439717, acc.: 50.20%] [G loss: 0.593742] [Loss change: 0.594, Loss increases: 0]\n",
      "100 [D loss: 0.746919, acc.: 50.00%] [G loss: 0.315994] [Loss change: 0.001, Loss increases: 0]\n",
      "200 [D loss: 0.773745, acc.: 49.41%] [G loss: 0.284848] [Loss change: -0.014, Loss increases: 2]\n",
      "300 [D loss: 0.785311, acc.: 49.80%] [G loss: 0.282601] [Loss change: -0.006, Loss increases: 0]\n",
      "400 [D loss: 0.789223, acc.: 49.80%] [G loss: 0.280772] [Loss change: -0.015, Loss increases: 1]\n",
      "500 [D loss: 0.807048, acc.: 50.00%] [G loss: 0.270989] [Loss change: -0.004, Loss increases: 0]\n",
      "Stoping on iteration:  554\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  4.  0.  0.  3.  0.  0.  2.  0.  1.  0.  0.  0.  0.  3.  0.\n",
      "   5.  4.  1.  0.  8.  8.  0.  0.  2.  0.  2.  0.  3.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  3.  0.  8.  0.  0.  2.  1.  0.  4.  1.  5.  0.  0.  0.  0.  5.  0.\n",
      "   6.  5.  1.  0.  0.  0.  0.  0.  0.  0.  7.  0.  3.  0.  0.  0.  0.  0.\n",
      "   0.  0.  8.  0. 11.]]\n",
      "[0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 1 0 1 0 1 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 1 1 0 0 1 0 0 1 1 0 0 1 0 0 1 0 0 1 0 0\n",
      " 1 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0 0 1 1 1 1 1 0\n",
      " 1 0 0 1 0 1 0 0 0 1 1 1 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 1 1 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 1 1\n",
      " 1 0 0 1 0 0 1 0 1 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1]\n",
      "Attack type: normal.     number predicted:  165\n",
      "Attack type: smurf.     number predicted:  91\n",
      "\n",
      "[[  0   0]\n",
      " [165  91]]\n",
      "total: 256\n",
      "accuracy: 0.35546875\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 25 25\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 2.162971, acc.: 49.80%] [G loss: 0.931620] [Loss change: 0.932, Loss increases: 0]\n",
      "100 [D loss: 0.515650, acc.: 53.12%] [G loss: 0.492335] [Loss change: 0.003, Loss increases: 0]\n",
      "200 [D loss: 0.835315, acc.: 50.00%] [G loss: 0.283185] [Loss change: -0.018, Loss increases: 1]\n",
      "300 [D loss: 0.935538, acc.: 50.00%] [G loss: 0.227232] [Loss change: -0.007, Loss increases: 2]\n",
      "400 [D loss: 0.991955, acc.: 50.00%] [G loss: 0.208002] [Loss change: -0.020, Loss increases: 1]\n",
      "500 [D loss: 1.024114, acc.: 50.00%] [G loss: 0.194830] [Loss change: -0.011, Loss increases: 0]\n",
      "600 [D loss: 1.049421, acc.: 50.00%] [G loss: 0.200060] [Loss change: -0.007, Loss increases: 2]\n",
      "700 [D loss: 1.095004, acc.: 50.00%] [G loss: 0.200482] [Loss change: 0.003, Loss increases: 0]\n",
      "800 [D loss: 1.035188, acc.: 49.22%] [G loss: 0.200701] [Loss change: -0.008, Loss increases: 2]\n",
      "900 [D loss: 1.024036, acc.: 49.61%] [G loss: 0.197267] [Loss change: -0.002, Loss increases: 1]\n",
      "1000 [D loss: 1.060334, acc.: 48.63%] [G loss: 0.200779] [Loss change: -0.022, Loss increases: 1]\n",
      "1100 [D loss: 1.046584, acc.: 50.20%] [G loss: 0.195508] [Loss change: 0.000, Loss increases: 0]\n",
      "1200 [D loss: 1.081149, acc.: 49.22%] [G loss: 0.195846] [Loss change: 0.009, Loss increases: 0]\n",
      "1300 [D loss: 1.022658, acc.: 48.44%] [G loss: 0.198476] [Loss change: 0.001, Loss increases: 1]\n",
      "1400 [D loss: 1.116012, acc.: 49.22%] [G loss: 0.197295] [Loss change: 0.016, Loss increases: 0]\n",
      "1500 [D loss: 1.135811, acc.: 48.83%] [G loss: 0.174160] [Loss change: -0.022, Loss increases: 1]\n",
      "1600 [D loss: 1.117952, acc.: 49.22%] [G loss: 0.172893] [Loss change: -0.015, Loss increases: 0]\n",
      "1700 [D loss: 1.110652, acc.: 49.61%] [G loss: 0.182913] [Loss change: 0.001, Loss increases: 1]\n",
      "1800 [D loss: 1.139120, acc.: 48.83%] [G loss: 0.188343] [Loss change: 0.010, Loss increases: 0]\n",
      "1900 [D loss: 1.156579, acc.: 49.02%] [G loss: 0.175021] [Loss change: -0.001, Loss increases: 1]\n",
      "2000 [D loss: 1.201087, acc.: 49.61%] [G loss: 0.186261] [Loss change: 0.003, Loss increases: 1]\n",
      "2100 [D loss: 1.201533, acc.: 49.80%] [G loss: 0.191318] [Loss change: 0.009, Loss increases: 2]\n",
      "2200 [D loss: 1.169309, acc.: 50.59%] [G loss: 0.181765] [Loss change: 0.001, Loss increases: 0]\n",
      "2300 [D loss: 1.179738, acc.: 50.20%] [G loss: 0.179205] [Loss change: -0.000, Loss increases: 2]\n",
      "2400 [D loss: 1.153519, acc.: 49.80%] [G loss: 0.183459] [Loss change: -0.004, Loss increases: 1]\n",
      "2500 [D loss: 1.116100, acc.: 50.39%] [G loss: 0.190753] [Loss change: 0.008, Loss increases: 0]\n",
      "2600 [D loss: 1.180980, acc.: 51.17%] [G loss: 0.179534] [Loss change: -0.004, Loss increases: 1]\n",
      "2700 [D loss: 1.282362, acc.: 50.39%] [G loss: 0.164439] [Loss change: 0.005, Loss increases: 1]\n",
      "2800 [D loss: 1.238784, acc.: 50.78%] [G loss: 0.164689] [Loss change: -0.005, Loss increases: 2]\n",
      "2900 [D loss: 1.244558, acc.: 50.20%] [G loss: 0.155703] [Loss change: 0.005, Loss increases: 0]\n",
      "3000 [D loss: 1.246904, acc.: 50.00%] [G loss: 0.146146] [Loss change: -0.003, Loss increases: 0]\n",
      "3100 [D loss: 1.258841, acc.: 49.80%] [G loss: 0.146288] [Loss change: -0.004, Loss increases: 3]\n",
      "3200 [D loss: 1.235210, acc.: 50.20%] [G loss: 0.157939] [Loss change: -0.013, Loss increases: 2]\n",
      "3300 [D loss: 1.228163, acc.: 49.80%] [G loss: 0.157863] [Loss change: -0.007, Loss increases: 1]\n",
      "3400 [D loss: 1.263939, acc.: 49.80%] [G loss: 0.153878] [Loss change: -0.004, Loss increases: 2]\n",
      "3500 [D loss: 1.235419, acc.: 49.80%] [G loss: 0.144191] [Loss change: 0.000, Loss increases: 0]\n",
      "3600 [D loss: 1.245433, acc.: 50.00%] [G loss: 0.156621] [Loss change: 0.004, Loss increases: 1]\n",
      "3700 [D loss: 1.254466, acc.: 49.80%] [G loss: 0.146559] [Loss change: 0.006, Loss increases: 0]\n",
      "3800 [D loss: 1.267181, acc.: 49.80%] [G loss: 0.147069] [Loss change: -0.010, Loss increases: 1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3900 [D loss: 1.271728, acc.: 50.20%] [G loss: 0.149580] [Loss change: 0.004, Loss increases: 0]\n",
      "4000 [D loss: 1.237411, acc.: 50.59%] [G loss: 0.162733] [Loss change: 0.012, Loss increases: 0]\n",
      "4100 [D loss: 1.256129, acc.: 50.59%] [G loss: 0.161857] [Loss change: 0.002, Loss increases: 0]\n",
      "4200 [D loss: 1.254623, acc.: 50.20%] [G loss: 0.159809] [Loss change: 0.002, Loss increases: 1]\n",
      "4300 [D loss: 1.163949, acc.: 50.20%] [G loss: 0.204209] [Loss change: 0.001, Loss increases: 0]\n",
      "4400 [D loss: 1.240158, acc.: 50.00%] [G loss: 0.183845] [Loss change: -0.002, Loss increases: 4]\n",
      "4500 [D loss: 1.229719, acc.: 50.39%] [G loss: 0.167192] [Loss change: -0.006, Loss increases: 0]\n",
      "4600 [D loss: 1.213512, acc.: 51.17%] [G loss: 0.182493] [Loss change: 0.002, Loss increases: 0]\n",
      "4700 [D loss: 1.179009, acc.: 51.37%] [G loss: 0.175984] [Loss change: -0.005, Loss increases: 2]\n",
      "Stoping on iteration:  4738\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0. 55. 57.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0. 63. 73.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1]\n",
      "Attack type: normal.     number predicted:  18\n",
      "Attack type: smurf.     number predicted:  238\n",
      "\n",
      "[[  0   0]\n",
      " [ 18 238]]\n",
      "total: 256\n",
      "accuracy: 0.9296875\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.470653, acc.: 54.49%] [G loss: 0.602956] [Loss change: 0.603, Loss increases: 0]\n",
      "100 [D loss: 0.724009, acc.: 50.98%] [G loss: 0.358214] [Loss change: -0.001, Loss increases: 0]\n",
      "200 [D loss: 0.821865, acc.: 50.59%] [G loss: 0.304797] [Loss change: -0.002, Loss increases: 1]\n",
      "300 [D loss: 0.807816, acc.: 50.59%] [G loss: 0.297516] [Loss change: -0.004, Loss increases: 1]\n",
      "400 [D loss: 0.842528, acc.: 50.00%] [G loss: 0.290178] [Loss change: 0.001, Loss increases: 1]\n",
      "500 [D loss: 0.825771, acc.: 49.80%] [G loss: 0.279493] [Loss change: -0.006, Loss increases: 0]\n",
      "600 [D loss: 0.817205, acc.: 49.02%] [G loss: 0.282676] [Loss change: -0.001, Loss increases: 0]\n",
      "700 [D loss: 0.819209, acc.: 49.61%] [G loss: 0.287676] [Loss change: 0.008, Loss increases: 0]\n",
      "800 [D loss: 0.807197, acc.: 50.00%] [G loss: 0.288773] [Loss change: -0.007, Loss increases: 3]\n",
      "900 [D loss: 0.861669, acc.: 49.41%] [G loss: 0.283888] [Loss change: -0.000, Loss increases: 0]\n",
      "1000 [D loss: 0.860037, acc.: 49.80%] [G loss: 0.286293] [Loss change: 0.007, Loss increases: 0]\n",
      "1100 [D loss: 0.880966, acc.: 50.39%] [G loss: 0.280484] [Loss change: -0.013, Loss increases: 1]\n",
      "1200 [D loss: 0.867864, acc.: 50.00%] [G loss: 0.280473] [Loss change: 0.008, Loss increases: 1]\n",
      "1300 [D loss: 0.910106, acc.: 50.20%] [G loss: 0.266253] [Loss change: -0.004, Loss increases: 1]\n",
      "1400 [D loss: 0.914398, acc.: 49.41%] [G loss: 0.257284] [Loss change: -0.006, Loss increases: 0]\n",
      "1500 [D loss: 0.920262, acc.: 50.59%] [G loss: 0.271852] [Loss change: 0.010, Loss increases: 0]\n",
      "1600 [D loss: 0.954142, acc.: 50.78%] [G loss: 0.262407] [Loss change: 0.008, Loss increases: 1]\n",
      "1700 [D loss: 0.965055, acc.: 50.59%] [G loss: 0.260270] [Loss change: 0.007, Loss increases: 1]\n",
      "1800 [D loss: 0.976785, acc.: 50.00%] [G loss: 0.252401] [Loss change: 0.009, Loss increases: 0]\n",
      "1900 [D loss: 0.998941, acc.: 50.39%] [G loss: 0.238431] [Loss change: -0.008, Loss increases: 2]\n",
      "2000 [D loss: 1.037501, acc.: 49.41%] [G loss: 0.237674] [Loss change: 0.000, Loss increases: 0]\n",
      "2100 [D loss: 1.064057, acc.: 49.61%] [G loss: 0.225533] [Loss change: -0.008, Loss increases: 0]\n",
      "2200 [D loss: 1.036647, acc.: 49.41%] [G loss: 0.222993] [Loss change: -0.002, Loss increases: 1]\n",
      "2300 [D loss: 1.058393, acc.: 49.02%] [G loss: 0.229469] [Loss change: 0.005, Loss increases: 2]\n",
      "2400 [D loss: 1.054828, acc.: 49.41%] [G loss: 0.228090] [Loss change: -0.016, Loss increases: 3]\n",
      "2500 [D loss: 1.102438, acc.: 48.44%] [G loss: 0.220821] [Loss change: 0.007, Loss increases: 0]\n",
      "2600 [D loss: 1.113776, acc.: 49.61%] [G loss: 0.219369] [Loss change: -0.007, Loss increases: 0]\n",
      "2700 [D loss: 1.093806, acc.: 49.61%] [G loss: 0.220854] [Loss change: -0.001, Loss increases: 0]\n",
      "2800 [D loss: 1.156025, acc.: 50.00%] [G loss: 0.220091] [Loss change: -0.011, Loss increases: 1]\n",
      "2900 [D loss: 1.136640, acc.: 51.56%] [G loss: 0.234428] [Loss change: 0.011, Loss increases: 0]\n",
      "3000 [D loss: 1.157529, acc.: 51.95%] [G loss: 0.226959] [Loss change: 0.006, Loss increases: 0]\n",
      "3100 [D loss: 1.152677, acc.: 51.17%] [G loss: 0.221353] [Loss change: 0.003, Loss increases: 1]\n",
      "3200 [D loss: 1.185790, acc.: 50.78%] [G loss: 0.220502] [Loss change: -0.009, Loss increases: 1]\n",
      "3300 [D loss: 1.194453, acc.: 52.15%] [G loss: 0.221853] [Loss change: 0.010, Loss increases: 1]\n",
      "3400 [D loss: 1.188126, acc.: 53.52%] [G loss: 0.218774] [Loss change: -0.005, Loss increases: 3]\n",
      "3500 [D loss: 1.283288, acc.: 50.59%] [G loss: 0.209841] [Loss change: 0.006, Loss increases: 0]\n",
      "3600 [D loss: 1.245259, acc.: 53.71%] [G loss: 0.217373] [Loss change: -0.003, Loss increases: 3]\n",
      "3700 [D loss: 1.269588, acc.: 51.56%] [G loss: 0.226210] [Loss change: 0.013, Loss increases: 0]\n",
      "3800 [D loss: 1.353681, acc.: 52.54%] [G loss: 0.197263] [Loss change: 0.002, Loss increases: 0]\n",
      "3900 [D loss: 1.332836, acc.: 52.54%] [G loss: 0.185296] [Loss change: -0.022, Loss increases: 1]\n",
      "4000 [D loss: 1.332277, acc.: 51.56%] [G loss: 0.194620] [Loss change: -0.011, Loss increases: 1]\n",
      "4100 [D loss: 1.314072, acc.: 52.54%] [G loss: 0.191223] [Loss change: 0.002, Loss increases: 0]\n",
      "4200 [D loss: 1.295444, acc.: 53.91%] [G loss: 0.190762] [Loss change: -0.012, Loss increases: 1]\n",
      "4300 [D loss: 1.293418, acc.: 53.71%] [G loss: 0.194067] [Loss change: -0.005, Loss increases: 0]\n",
      "4400 [D loss: 1.294748, acc.: 52.93%] [G loss: 0.189213] [Loss change: -0.005, Loss increases: 0]\n",
      "4500 [D loss: 1.264572, acc.: 54.10%] [G loss: 0.197422] [Loss change: -0.012, Loss increases: 2]\n",
      "4600 [D loss: 1.336699, acc.: 54.30%] [G loss: 0.199078] [Loss change: 0.009, Loss increases: 0]\n",
      "4700 [D loss: 1.324563, acc.: 54.10%] [G loss: 0.190357] [Loss change: -0.017, Loss increases: 1]\n",
      "4800 [D loss: 1.284474, acc.: 53.91%] [G loss: 0.202572] [Loss change: -0.004, Loss increases: 0]\n",
      "4900 [D loss: 1.294195, acc.: 51.76%] [G loss: 0.197760] [Loss change: 0.002, Loss increases: 0]\n",
      "5000 [D loss: 1.350150, acc.: 51.56%] [G loss: 0.207412] [Loss change: 0.006, Loss increases: 1]\n",
      "5100 [D loss: 1.332126, acc.: 53.71%] [G loss: 0.211131] [Loss change: 0.012, Loss increases: 1]\n",
      "5200 [D loss: 1.310633, acc.: 51.37%] [G loss: 0.197829] [Loss change: 0.005, Loss increases: 0]\n",
      "5300 [D loss: 1.263541, acc.: 51.56%] [G loss: 0.195704] [Loss change: 0.008, Loss increases: 1]\n",
      "5400 [D loss: 1.223915, acc.: 52.34%] [G loss: 0.201018] [Loss change: 0.004, Loss increases: 0]\n",
      "5500 [D loss: 1.218395, acc.: 50.59%] [G loss: 0.197498] [Loss change: -0.004, Loss increases: 1]\n",
      "5600 [D loss: 1.187575, acc.: 50.39%] [G loss: 0.201826] [Loss change: -0.005, Loss increases: 1]\n",
      "5700 [D loss: 1.192767, acc.: 50.39%] [G loss: 0.210455] [Loss change: 0.001, Loss increases: 1]\n",
      "5800 [D loss: 1.137302, acc.: 50.20%] [G loss: 0.193840] [Loss change: -0.004, Loss increases: 0]\n",
      "5900 [D loss: 1.089624, acc.: 50.59%] [G loss: 0.215871] [Loss change: -0.000, Loss increases: 2]\n",
      "6000 [D loss: 1.073654, acc.: 50.98%] [G loss: 0.219052] [Loss change: 0.007, Loss increases: 1]\n",
      "6100 [D loss: 1.088478, acc.: 50.78%] [G loss: 0.219136] [Loss change: -0.009, Loss increases: 4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6200 [D loss: 1.079033, acc.: 50.39%] [G loss: 0.218363] [Loss change: -0.001, Loss increases: 0]\n",
      "6300 [D loss: 1.052607, acc.: 50.39%] [G loss: 0.228840] [Loss change: 0.005, Loss increases: 1]\n",
      "6400 [D loss: 1.034466, acc.: 50.59%] [G loss: 0.230449] [Loss change: 0.010, Loss increases: 0]\n",
      "6500 [D loss: 1.027664, acc.: 50.00%] [G loss: 0.227739] [Loss change: 0.002, Loss increases: 0]\n",
      "6600 [D loss: 1.002580, acc.: 50.00%] [G loss: 0.234540] [Loss change: -0.001, Loss increases: 0]\n",
      "6700 [D loss: 0.994606, acc.: 50.00%] [G loss: 0.236368] [Loss change: 0.005, Loss increases: 0]\n",
      "6800 [D loss: 1.012600, acc.: 50.00%] [G loss: 0.238135] [Loss change: -0.003, Loss increases: 0]\n",
      "6900 [D loss: 0.956698, acc.: 50.00%] [G loss: 0.256798] [Loss change: -0.003, Loss increases: 0]\n",
      "Generated Smurf attacks: \n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  3.  0. 57. 57.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0. 36. 37.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.]]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Attack type: smurf.     number predicted:  256\n",
      "\n",
      "[[256]]\n",
      "total: 256\n",
      "accuracy: 1.0\n",
      "Real smurf attacks:\n",
      "[[ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 26 26\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]\n",
      "0 [D loss: 0.407583, acc.: 50.78%] [G loss: 0.572662] [Loss change: 0.573, Loss increases: 0]\n",
      "100 [D loss: 0.622988, acc.: 50.00%] [G loss: 0.367239] [Loss change: -0.003, Loss increases: 1]\n",
      "200 [D loss: 0.684495, acc.: 50.00%] [G loss: 0.325080] [Loss change: -0.001, Loss increases: 0]\n",
      "300 [D loss: 0.737547, acc.: 50.20%] [G loss: 0.318463] [Loss change: -0.001, Loss increases: 0]\n",
      "400 [D loss: 0.727355, acc.: 50.00%] [G loss: 0.311014] [Loss change: 0.000, Loss increases: 1]\n",
      "500 [D loss: 0.761547, acc.: 50.20%] [G loss: 0.296704] [Loss change: -0.005, Loss increases: 0]\n",
      "600 [D loss: 0.765976, acc.: 50.00%] [G loss: 0.305088] [Loss change: -0.005, Loss increases: 2]\n",
      "700 [D loss: 0.810728, acc.: 49.22%] [G loss: 0.289475] [Loss change: -0.007, Loss increases: 2]\n",
      "800 [D loss: 0.804364, acc.: 50.00%] [G loss: 0.293286] [Loss change: 0.002, Loss increases: 1]\n",
      "900 [D loss: 0.838917, acc.: 50.00%] [G loss: 0.288185] [Loss change: 0.002, Loss increases: 0]\n",
      "1000 [D loss: 0.894458, acc.: 50.20%] [G loss: 0.264930] [Loss change: 0.002, Loss increases: 0]\n",
      "1100 [D loss: 0.902430, acc.: 50.00%] [G loss: 0.267211] [Loss change: 0.012, Loss increases: 0]\n",
      "1200 [D loss: 0.923585, acc.: 50.00%] [G loss: 0.259911] [Loss change: -0.003, Loss increases: 1]\n",
      "1300 [D loss: 0.912602, acc.: 50.20%] [G loss: 0.253567] [Loss change: 0.000, Loss increases: 2]\n",
      "1400 [D loss: 0.958135, acc.: 50.39%] [G loss: 0.245003] [Loss change: -0.005, Loss increases: 1]\n",
      "1500 [D loss: 0.951871, acc.: 50.20%] [G loss: 0.233883] [Loss change: -0.001, Loss increases: 1]\n",
      "1600 [D loss: 1.028281, acc.: 50.20%] [G loss: 0.222148] [Loss change: -0.010, Loss increases: 1]\n",
      "1700 [D loss: 1.010971, acc.: 50.39%] [G loss: 0.230583] [Loss change: 0.008, Loss increases: 2]\n",
      "1800 [D loss: 1.009199, acc.: 50.00%] [G loss: 0.215042] [Loss change: 0.005, Loss increases: 0]\n",
      "1900 [D loss: 1.031538, acc.: 50.00%] [G loss: 0.213606] [Loss change: -0.001, Loss increases: 1]\n",
      "2000 [D loss: 1.020030, acc.: 49.61%] [G loss: 0.214196] [Loss change: -0.005, Loss increases: 0]\n",
      "2100 [D loss: 1.006852, acc.: 49.80%] [G loss: 0.221097] [Loss change: -0.002, Loss increases: 1]\n",
      "2200 [D loss: 0.986228, acc.: 49.61%] [G loss: 0.242285] [Loss change: 0.009, Loss increases: 2]\n",
      "2300 [D loss: 0.997225, acc.: 49.80%] [G loss: 0.230236] [Loss change: 0.000, Loss increases: 0]\n",
      "2400 [D loss: 0.978471, acc.: 50.00%] [G loss: 0.236836] [Loss change: 0.004, Loss increases: 0]\n",
      "2500 [D loss: 0.952026, acc.: 49.80%] [G loss: 0.244231] [Loss change: -0.000, Loss increases: 0]\n",
      "2600 [D loss: 0.952594, acc.: 49.41%] [G loss: 0.257845] [Loss change: -0.003, Loss increases: 0]\n",
      "2700 [D loss: 0.967349, acc.: 49.41%] [G loss: 0.262531] [Loss change: 0.011, Loss increases: 0]\n",
      "2800 [D loss: 0.968213, acc.: 48.83%] [G loss: 0.269056] [Loss change: -0.009, Loss increases: 1]\n",
      "2900 [D loss: 1.025737, acc.: 49.80%] [G loss: 0.277553] [Loss change: -0.002, Loss increases: 0]\n",
      "3000 [D loss: 0.945919, acc.: 49.41%] [G loss: 0.280779] [Loss change: -0.011, Loss increases: 3]\n",
      "3100 [D loss: 0.925123, acc.: 49.80%] [G loss: 0.297742] [Loss change: 0.005, Loss increases: 0]\n",
      "3200 [D loss: 0.897701, acc.: 49.80%] [G loss: 0.323844] [Loss change: 0.006, Loss increases: 1]\n",
      "3300 [D loss: 0.917685, acc.: 49.80%] [G loss: 0.320107] [Loss change: -0.004, Loss increases: 0]\n",
      "3400 [D loss: 0.931546, acc.: 50.00%] [G loss: 0.350888] [Loss change: 0.007, Loss increases: 0]\n",
      "3500 [D loss: 0.924485, acc.: 50.00%] [G loss: 0.341362] [Loss change: 0.008, Loss increases: 0]\n",
      "3600 [D loss: 0.889770, acc.: 50.39%] [G loss: 0.346795] [Loss change: -0.012, Loss increases: 3]\n",
      "3700 [D loss: 0.843386, acc.: 50.98%] [G loss: 0.376292] [Loss change: 0.015, Loss increases: 0]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "f = open(\"GeneratorHypersAbove50percentAccuracy.txt\", \"w\")\n",
    "f.write(\"\"\"\"\"\" Hidden layer counts for Generator model that resulted in over 50% generated attacks labeled correctly:\n",
    "    ------------------------------------------------------------------------------------------------\n",
    "    \"\"\"\"\"\")\n",
    "f.close()\n",
    "\"\"\"\n",
    "\n",
    "while(1):\n",
    "    # generate random numbers for the hidden layer sizes of our generator\n",
    "    gen_hidden1 = np.random.randint(1, 101)\n",
    "    gen_hidden2 = np.random.randint(1, 101)\n",
    "    gen_hidden3 = np.random.randint(1, 101)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    \n",
    "    # train 5 times on each setup, in case we get unlucky initalization on an otherwise good setup\n",
    "    while i < 5:\n",
    "        # create a unique filename in case we want to store the results (good accuracy)\n",
    "        result_filename = \"GANresultsSmurf%.0f%.0f%.0fiter%.0f.txt\" % (gen_hidden1, gen_hidden2, gen_hidden3, i)\n",
    "\n",
    "        trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "        \n",
    "        # load generate attacks from file\n",
    "        results = np.loadtxt(\"GANresultsSmurf.txt\")\n",
    "\n",
    "        # predict attack lables (as encoded integers)\n",
    "        y_pred = estimator.predict(results)\n",
    "        print(y_pred)\n",
    "\n",
    "        # create appropriate labels for our generated smurf attacks\n",
    "        smurf_labels = np.full((len(results),), smurf_index[0])\n",
    "\n",
    "        # convert integer labels back to string, get all unique strings and their count\n",
    "        predicted_as_label = attack_labels[y_pred]\n",
    "        unique_labels = np.unique(predicted_as_label)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            print(\"Attack type: %s     number predicted:  %.0f\" % (label, len(np.where(predicted_as_label == label)[0])))\n",
    "    \n",
    "        print()\n",
    "        # create a confusion matrix of the results\n",
    "        cm = confusion_matrix(smurf_labels, y_pred)\n",
    "        \n",
    "        accuracy = np.trace(cm) / cm.sum()\n",
    "        print(cm)\n",
    "        print(\"total: \" + str(cm.sum()))\n",
    "        print(\"accuracy: \" + str(accuracy))\n",
    "        \n",
    "        if accuracy > .50:\n",
    "            f = open(\"GeneratorHypersAbove50percentAccuracySmurf.txt\", \"a\")\n",
    "            f.write(\"\"\"\n",
    "            \n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "Accuracy: %.3f\n",
    "Generator hidden layer 1 size: %.0f\n",
    "Generator hidden layer 2 size: %.0f\n",
    "Generator hidden layer 3 size: %.0f\n",
    "Iteration %.0f\n",
    "Result file name: %s\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\"\"\" % (accuracy, gen_hidden1, gen_hidden2, gen_hidden3, i, result_filename))\n",
    "            f.close()\n",
    "            \n",
    "            f = open(result_filename, \"w\")\n",
    "            f.close()\n",
    "            np.savetxt(result_filename, results, fmt=\"%.0f\")\n",
    "        \n",
    "        i = i + 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
