{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.contrib.data.python.ops.readers.CsvDataset'>\n",
      "<CsvDataset shapes: ((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()), types: (tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    " #Creates a dataset that reads all of the records from two CSV files, each with\n",
    " #eight float columns\n",
    "filenames = [\"kddforpandatest.csv\", \"kddforpandatrain.csv\"]\n",
    "record_defaults = [tf.float32] * 42   # 42 columns, not all floats, but did this for simplicity purposes\n",
    "kdd = tf.contrib.data.CsvDataset(filenames, record_defaults)\n",
    "\n",
    "print(type(kdd))\n",
    "print(kdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Metadata describing the text columns\n",
    "COLUMNS = ['duration','protocol_type','service','flag','src_bytes','dst_bytes','land','wrong_fragment','urgent',\n",
    "           'hot','num_failed_logins','logged_in','num_compromised','root_shell','su_attempted','num_root','num_file_creations',\n",
    "           'num_shells','num_access_files','num_outbound_cmds','is_host_login','is_guest_login','count','srv_count',\n",
    "           'serror_rate','srv_serror_rate','rerror_rate','srv_rerror_rate','same_srv_rate','diff_srv_rate','srv_diff_host_rate',\n",
    "           'dst_host_count','dst_host_srv_count','dst_host_same_srv_rate','dst_host_diff_srv_rate','dst_host_same_src_port_rate',\n",
    "           'dst_host_srv_diff_host_rate','dst_host_serror_rate','dst_host_srv_serror_rate','dst_host_rerror_rate',\n",
    "           'dst_host_srv_rerror_rate','attack_type']\n",
    "print(len(COLUMNS))\n",
    "FIELD_DEFAULTS = [[0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],\n",
    "                  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],\n",
    "                  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],\n",
    "                  [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0], [0.0],\n",
    "                  [0.0], [0.0]]\n",
    "print(len(FIELD_DEFAULTS))\n",
    "def _parse_line(line):\n",
    "    # Decode the line into its fields\n",
    "    fields = tf.decode_csv(line, FIELD_DEFAULTS)\n",
    "\n",
    "    # Pack the result into a dictionary\n",
    "    features = dict(zip(COLUMNS,fields))\n",
    "\n",
    "    # Separate the label from the features\n",
    "    label = features.pop('attack_type')\n",
    "\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TextLineDataset(\"kddforpandatest.csv\").skip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MapDataset shapes: ({duration: (), protocol_type: (), service: (), flag: (), src_bytes: (), dst_bytes: (), land: (), wrong_fragment: (), urgent: (), hot: (), num_failed_logins: (), logged_in: (), num_compromised: (), root_shell: (), su_attempted: (), num_root: (), num_file_creations: (), num_shells: (), num_access_files: (), num_outbound_cmds: (), is_host_login: (), is_guest_login: (), count: (), srv_count: (), serror_rate: (), srv_serror_rate: (), rerror_rate: (), srv_rerror_rate: (), same_srv_rate: (), diff_srv_rate: (), srv_diff_host_rate: (), dst_host_count: (), dst_host_srv_count: (), dst_host_same_srv_rate: (), dst_host_diff_srv_rate: (), dst_host_same_src_port_rate: (), dst_host_srv_diff_host_rate: (), dst_host_serror_rate: (), dst_host_srv_serror_rate: (), dst_host_rerror_rate: (), dst_host_srv_rerror_rate: ()}, ()), types: ({duration: tf.float32, protocol_type: tf.float32, service: tf.float32, flag: tf.float32, src_bytes: tf.float32, dst_bytes: tf.float32, land: tf.float32, wrong_fragment: tf.float32, urgent: tf.float32, hot: tf.float32, num_failed_logins: tf.float32, logged_in: tf.float32, num_compromised: tf.float32, root_shell: tf.float32, su_attempted: tf.float32, num_root: tf.float32, num_file_creations: tf.float32, num_shells: tf.float32, num_access_files: tf.float32, num_outbound_cmds: tf.float32, is_host_login: tf.float32, is_guest_login: tf.float32, count: tf.float32, srv_count: tf.float32, serror_rate: tf.float32, srv_serror_rate: tf.float32, rerror_rate: tf.float32, srv_rerror_rate: tf.float32, same_srv_rate: tf.float32, diff_srv_rate: tf.float32, srv_diff_host_rate: tf.float32, dst_host_count: tf.float32, dst_host_srv_count: tf.float32, dst_host_same_srv_rate: tf.float32, dst_host_diff_srv_rate: tf.float32, dst_host_same_src_port_rate: tf.float32, dst_host_srv_diff_host_rate: tf.float32, dst_host_serror_rate: tf.float32, dst_host_srv_serror_rate: tf.float32, dst_host_rerror_rate: tf.float32, dst_host_srv_rerror_rate: tf.float32}, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "ds = ds.map(_parse_line)\n",
    "print(ds)\n",
    "#print(type(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"CSV_PATH = 'kddforpandatest.csv'\\ndataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)\\niter = dataset.make_one_shot_iterator()\\nnext = iter.get_next()\\nprint(next) # next is a dict with key=columns names and value=column data\\nprint(type(dataset))\\n#inputs, labels = next,next['attack_type']\\n#with  tf.Session() as sess:\\n #   sess.run([inputs, labels])\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CSV_PATH = 'kddforpandatest.csv'\n",
    "dataset = tf.contrib.data.make_csv_dataset(CSV_PATH, batch_size=32)\n",
    "iter = dataset.make_one_shot_iterator()\n",
    "next = iter.get_next()\n",
    "print(next) # next is a dict with key=columns names and value=column data\n",
    "print(type(dataset))\n",
    "#inputs, labels = next,next['attack_type']\n",
    "#with  tf.Session() as sess:\n",
    " #   sess.run([inputs, labels])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
