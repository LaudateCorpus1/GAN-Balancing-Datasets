{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator():\n",
    "    model = Sequential() #41 30 15\n",
    "    model.add(Dense(64, input_dim=41))  # discriminator takes 41 values from our dataset\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(32))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # outputs 0 to 1, 1 being real and 0 being fake\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(hidden1, hidden2, hidden3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(hidden1, input_dim=41))  # arbitrarily selected 100 for our input noise vector?\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(hidden2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(hidden3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(41, activation='relu'))  # outputs a generated vector of the same size as our data (41)\n",
    "\n",
    "    # model.summary()\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainGAN(gen_hidden1, gen_hidden2, gen_hidden3):\n",
    "    batch_size = 4999\n",
    "    epochs = 3001\n",
    "    optimizer = Adam(0.001)\n",
    "    \n",
    "    dataframe = pd.read_csv('../CSV/kdd_neptune_only_5000.csv').sample(4999)\n",
    "    \n",
    "    # apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    #to visually judge results\n",
    "    print(\"Real neptune attacks:\")\n",
    "    print(dataset[:2])\n",
    "    \n",
    "    # Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(float)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    # labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # build the discriminator portion\n",
    "    discriminator = build_discriminator();\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    # build the generator portion\n",
    "    generator = build_generator(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    # build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0;\n",
    "    prev_g_loss = 0;\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        \n",
    "        # selecting batch_size random attacks from our training data\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        idx = np.arange(4999)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        # generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        # create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        # loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 500 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "        #if our generator loss icreased this iteration, increment the counter by 1\n",
    "#         if (g_loss - prev_g_loss) > 0:\n",
    "#             loss_increase_count = loss_increase_count + 1\n",
    "#         else: \n",
    "#             loss_increase_count = 0  # otherwise, reset it to 0, we are still training effectively\n",
    "            \n",
    "#         prev_g_loss = g_loss\n",
    "            \n",
    "#         if loss_increase_count > 10:\n",
    "#             print('Stoping on iteration: ', epoch)\n",
    "#             break\n",
    "\n",
    "    # generate a matrix of noise vectors\n",
    "    noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "    # create an array of generated attacks\n",
    "    gen_attacks = generator.predict(noise)\n",
    "    f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "    np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%d\")\n",
    "    f.close()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['neptune.' 'normal.']\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Random Number Generator\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "# load dataset\n",
    "\n",
    "dataframe = pd.read_csv(\"../CSV/normalAndNeptune.csv\")\n",
    "\n",
    "# samples 10000 random data points from 500k\n",
    "dataframe = dataframe.sample(n=500000)\n",
    "# LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "print(attack_labels)\n",
    "print(neptune_index)\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(float)\n",
    "Y = dataset[:,41]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237615\n",
      "262385\n"
     ]
    }
   ],
   "source": [
    "print((Y == 1).sum())\n",
    "print((Y == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(encoded_Y)\n",
    "# print(dummy_y)\n",
    "#print(len(dummy_y[0]))\n",
    "num_of_classes = len(dummy_y[0])  # the length of dummy y is the number of classes we have in our small sample\n",
    "# since we are randomly sampling from a large dataset, we might not get 1 of every class in our sample\n",
    "# we need to set output layer to be equal to the length of our dummy_y vectors\n",
    "print(num_of_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get validation data\n",
    "validationToTrainRatio = 0.2\n",
    "validationSize = int(validationToTrainRatio * len(X))\n",
    "validationData = X[:validationSize]\n",
    "validationLabels = Y[:validationSize]\n",
    "X = X[validationSize:]\n",
    "Y = Y[validationSize:]\n",
    "\n",
    "#Get test data\n",
    "testToTrainRatio = 0.1\n",
    "testSize = int(testToTrainRatio * len(X))\n",
    "testData = X[:testSize]\n",
    "testLabels = Y[:testSize]\n",
    "X = X[testSize:]\n",
    "Y = Y[testSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171075\n",
      "188925\n"
     ]
    }
   ],
   "source": [
    "print((Y == 1).sum())\n",
    "print((Y == 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = keras.Sequential()\n",
    "    model.add(keras.layers.Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(keras.layers.Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(keras.layers.Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(keras.layers.Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360000 samples, validate on 100000 samples\n",
      "Epoch 1/40\n",
      " - 3s - loss: 1.8514 - acc: 0.8375 - val_loss: 0.0024 - val_acc: 0.9996\n",
      "Epoch 2/40\n",
      " - 2s - loss: 0.2091 - acc: 0.9586 - val_loss: 0.0023 - val_acc: 0.9998\n",
      "Epoch 3/40\n",
      " - 2s - loss: 0.0717 - acc: 0.9795 - val_loss: 0.0036 - val_acc: 0.9998\n",
      "Epoch 4/40\n",
      " - 2s - loss: 0.0534 - acc: 0.9843 - val_loss: 0.0040 - val_acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "estimator = baseline_model(layers=2, units=32, dropout_rate=0.5, input_shape=X.shape[1:], num_classes=1)\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = estimator.fit(X,\n",
    "                    Y,\n",
    "                    epochs=40,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(validationData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 1s 27us/step\n",
      "[0.0041814853875082915, 0.9999]\n"
     ]
    }
   ],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(estimator.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real neptune attacks:\n",
      "[[  0   0  38   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 174  17   1   3   0   0   8   2   0  25  17   7   2   0\n",
      "    0   0   0   0   0   0]\n",
      " [  0   0  38   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0 147  19   1   3   0   0   9   2   0  25  19   8   4   0\n",
      "    0   0   0   0   0   0]]\n",
      "0 [D loss: 4.258030, acc.: 46.81%] [G loss: 0.815551] [Loss change: 0.816, Loss increases: 0]\n",
      "500 [D loss: 2.062886, acc.: 50.51%] [G loss: 0.091353] [Loss change: 0.091, Loss increases: 0]\n",
      "1000 [D loss: 2.015170, acc.: 50.76%] [G loss: 0.128834] [Loss change: 0.129, Loss increases: 0]\n",
      "1500 [D loss: 1.885279, acc.: 51.58%] [G loss: 0.169821] [Loss change: 0.170, Loss increases: 0]\n",
      "2000 [D loss: 1.804629, acc.: 51.73%] [G loss: 0.208966] [Loss change: 0.209, Loss increases: 0]\n",
      "2500 [D loss: 1.660054, acc.: 53.35%] [G loss: 0.273184] [Loss change: 0.273, Loss increases: 0]\n",
      "3000 [D loss: 1.690592, acc.: 47.33%] [G loss: 0.287929] [Loss change: 0.288, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n"
     ]
    }
   ],
   "source": [
    "# generate random numbers for the hidden layer sizes of our generator\n",
    "gen_hidden1 = 16#np.random.randint(1, 101)\n",
    "gen_hidden2 = 32#np.random.randint(1, 101)\n",
    "gen_hidden3 = 64#np.random.randint(1, 101)\n",
    "    \n",
    "trainGAN(gen_hidden1, gen_hidden2, gen_hidden3)\n",
    "\n",
    "# predict attack lables (as encoded integers)\n",
    "results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "y_pred = estimator.predict(results)\n",
    "        \n",
    "right = float((y_pred == 1).sum())\n",
    "wrong = float(len(y_pred)-(y_pred == 1).sum())\n",
    "print(\"Number of right predictions: %d\" % right)\n",
    "print(\"Number of wrong predictions: %d\" % wrong)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
