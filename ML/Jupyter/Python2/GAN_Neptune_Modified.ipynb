{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(layer1, layer2, layer3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41)) #discriminator takes 41 values from our dataset\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid')) #outputs 0 to 1, 1 being real and 0 being fake\n",
    "\n",
    "    attack = Input(shape=(41,))\n",
    "    validity = model(attack)\n",
    "\n",
    "    return Model(attack, validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(layer1, layer2, layer3):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layer1, input_dim=41))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(layer2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(layer3))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(41, activation='relu'))\n",
    "\n",
    "    noise = Input(shape=(41,))\n",
    "    attack = model(noise)\n",
    "    return Model(noise, attack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_model(layer1, layer2, layer3):\n",
    "    optimizer = Adam(0.001)\n",
    "    \n",
    "    #build generator and discriminator (mirrored)\n",
    "    generator = build_generator(layer1, layer2, layer3)\n",
    "    \n",
    "    discriminator = build_discriminator(layer3, layer2, layer1)\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    \n",
    "    #input and output of our combined model\n",
    "    z = Input(shape=(41,))\n",
    "    attack = generator(z)\n",
    "    validity = discriminator(attack)\n",
    "    \n",
    "    #build combined model from generator and discriminator\n",
    "    combined = Model(z, validity)\n",
    "    combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    return combined, discriminator, generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(combined, discriminator, generator, epochs):\n",
    "    epochs = epochs\n",
    "    batch_size = 4999\n",
    "    dataframe = pd.read_csv('../CSV/kdd_neptune_only_5000.csv').sample(batch_size)\n",
    "    \n",
    "    #apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "    le = LabelEncoder()\n",
    "    dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "    dataset = dataframe_encoded.values\n",
    "    \n",
    "    #labels for data. 1 for valid attacks, 0 for fake (generated) attacks\n",
    "    valid = np.ones((batch_size, 1))\n",
    "    fake = np.zeros((batch_size, 1))\n",
    "    \n",
    "    #Set X as our input data and Y as our label\n",
    "    X_train = dataset[:, 0:41].astype(float)\n",
    "    Y_train = dataset[:, 41]\n",
    "    \n",
    "    #break condition for training (when diverging)\n",
    "    loss_increase_count = 0\n",
    "    prev_g_loss = 0\n",
    "    \n",
    "    #generating a np array of numbers 0..batch_size-1\n",
    "    idx = np.arange(batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        #selecting batch_size random attacks from our training data\n",
    "        #idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "        attacks = X_train[idx]\n",
    "        \n",
    "        #generate a matrix of noise vectors\n",
    "        noise = np.random.normal(0, 1, (batch_size, 41))\n",
    "        \n",
    "        #create an array of generated attacks\n",
    "        gen_attacks = generator.predict(noise)\n",
    "        \n",
    "        #loss functions, based on what metrics we specify at model compile time\n",
    "        d_loss_real = discriminator.train_on_batch(attacks, valid)\n",
    "        d_loss_fake = discriminator.train_on_batch(gen_attacks, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        #generator loss function\n",
    "        g_loss = combined.train_on_batch(noise, valid)\n",
    "        \n",
    "        if epoch % 50 == 0:\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f] [Loss change: %.3f, Loss increases: %.0f]\" % \n",
    "                  (epoch, d_loss[0], 100 * d_loss[1], g_loss, g_loss - prev_g_loss, loss_increase_count))\n",
    "        \n",
    "            #saving results to txt to track them as the gan is training\n",
    "            f = open(\"GANresultsNeptune.txt\", \"a\")\n",
    "            np.savetxt(\"GANresultsNeptune.txt\", gen_attacks, fmt=\"%d\")\n",
    "            f.close()\n",
    "        \n",
    "            results = np.loadtxt(\"GANresultsNeptune.txt\")\n",
    "            y_pred = estimator.predict(results)\n",
    "        \n",
    "            right = (y_pred == 1).sum()\n",
    "            wrong = len(y_pred)-(y_pred == 1).sum()\n",
    "            accuracy = (right/float(right+wrong))\n",
    "            print(\"Number of right predictions: %d\" % right)\n",
    "            print(\"Number of wrong predictions: %d\" % wrong)\n",
    "            print(\"Accuracy: %.4f \" % accuracy)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Random Number Generator\n",
    "#fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "#load dataset\n",
    "dataframe = pd.read_csv(\"../CSV/normalAndNeptune.csv\")\n",
    "\n",
    "#samples n random data points\n",
    "dataframe = dataframe.sample(n=1000000)\n",
    "#LabelEncoder, turns all our categorical data into integers\n",
    "le = LabelEncoder()\n",
    "\n",
    "#apply \"le.fit_transform\" to every column (usually only works on 1 column)\n",
    "dataframe_encoded = dataframe.apply(le.fit_transform)\n",
    "attack_labels = le.classes_\n",
    "indices_of_neptune = np.where(attack_labels == 'neptune.')\n",
    "neptune_index = indices_of_neptune[0]\n",
    "dataset = dataframe_encoded.values\n",
    "\n",
    "#Set X as our input data and Y as our label\n",
    "X = dataset[:,0:41].astype(float)\n",
    "Y = dataset[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get validation data\n",
    "validationToTrainRatio = 0.05\n",
    "validationSize = int(validationToTrainRatio * len(X))\n",
    "validationData = X[:validationSize]\n",
    "validationLabels = Y[:validationSize]\n",
    "X = X[validationSize:]\n",
    "Y = Y[validationSize:]\n",
    "\n",
    "#Get test data\n",
    "testToTrainRatio = 0.05\n",
    "testSize = int(testToTrainRatio * len(X))\n",
    "testData = X[:testSize]\n",
    "testLabels = Y[:testSize]\n",
    "X = X[testSize:]\n",
    "Y = Y[testSize:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model(layers, units, dropout_rate, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(rate=dropout_rate, input_shape=input_shape))\n",
    "    for _ in range(layers-1):\n",
    "        model.add(Dense(units=units, activation=tf.nn.relu))\n",
    "        model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "    model.add(Dense(units=num_classes, activation=tf.nn.sigmoid))\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 902500 samples, validate on 50000 samples\n",
      "Epoch 1/5\n",
      " - 6s - loss: 0.5924 - acc: 0.9302 - val_loss: 0.0039 - val_acc: 0.9999\n",
      "Epoch 2/5\n",
      " - 5s - loss: 0.0470 - acc: 0.9861 - val_loss: 0.0065 - val_acc: 0.9998\n",
      "Epoch 3/5\n",
      " - 5s - loss: 0.0325 - acc: 0.9898 - val_loss: 0.0034 - val_acc: 0.9999\n",
      "Epoch 4/5\n",
      " - 5s - loss: 0.0277 - acc: 0.9909 - val_loss: 0.0014 - val_acc: 0.9999\n",
      "Epoch 5/5\n",
      " - 5s - loss: 0.0246 - acc: 0.9917 - val_loss: 0.0011 - val_acc: 0.9999\n"
     ]
    }
   ],
   "source": [
    "estimator = baseline_model(layers=2, units=32, dropout_rate=0.5, input_shape=X.shape[1:], num_classes=1)\n",
    "\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=2)]\n",
    "\n",
    "history = estimator.fit(X,\n",
    "                    Y,\n",
    "                    epochs=5,\n",
    "                    batch_size=1024,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=(validationData, validationLabels),\n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47500/47500 [==============================] - 1s 23us/step\n",
      "[0.001588432508278148, 0.9998947368421053]\n"
     ]
    }
   ],
   "source": [
    "#Evalueating model on the testset\n",
    "#[loss, accuracy]\n",
    "print(estimator.evaluate(testData, testLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#creating GAN model\n",
    "combined, discriminator, generator = GAN_model(8, 16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 2.912833, acc.: 47.12%] [G loss: 0.773026] [Loss change: 0.773, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "50 [D loss: 1.059959, acc.: 50.19%] [G loss: 0.201338] [Loss change: 0.201, Loss increases: 0]\n",
      "Number of right predictions: 10\n",
      "Number of wrong predictions: 4989\n",
      "Accuracy: 0.0020 \n",
      "100 [D loss: 1.275534, acc.: 50.31%] [G loss: 0.134777] [Loss change: 0.135, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "150 [D loss: 1.429064, acc.: 50.40%] [G loss: 0.117276] [Loss change: 0.117, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "200 [D loss: 1.550088, acc.: 50.72%] [G loss: 0.108939] [Loss change: 0.109, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "250 [D loss: 1.587757, acc.: 50.69%] [G loss: 0.113296] [Loss change: 0.113, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "300 [D loss: 1.591069, acc.: 50.93%] [G loss: 0.109122] [Loss change: 0.109, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "350 [D loss: 1.621058, acc.: 50.81%] [G loss: 0.120346] [Loss change: 0.120, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "400 [D loss: 1.654686, acc.: 50.67%] [G loss: 0.116955] [Loss change: 0.117, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "450 [D loss: 1.676980, acc.: 50.80%] [G loss: 0.120844] [Loss change: 0.121, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "500 [D loss: 1.740984, acc.: 50.86%] [G loss: 0.134427] [Loss change: 0.134, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "550 [D loss: 1.763581, acc.: 50.94%] [G loss: 0.138647] [Loss change: 0.139, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "600 [D loss: 1.785596, acc.: 50.67%] [G loss: 0.133768] [Loss change: 0.134, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "650 [D loss: 1.796049, acc.: 50.67%] [G loss: 0.137939] [Loss change: 0.138, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "700 [D loss: 1.774168, acc.: 51.10%] [G loss: 0.146937] [Loss change: 0.147, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "750 [D loss: 1.802205, acc.: 51.15%] [G loss: 0.147243] [Loss change: 0.147, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "800 [D loss: 1.738572, acc.: 51.33%] [G loss: 0.147933] [Loss change: 0.148, Loss increases: 0]\n",
      "Number of right predictions: 0\n",
      "Number of wrong predictions: 4999\n",
      "Accuracy: 0.0000 \n",
      "850 [D loss: 1.735177, acc.: 50.80%] [G loss: 0.161954] [Loss change: 0.162, Loss increases: 0]\n",
      "Number of right predictions: 2251\n",
      "Number of wrong predictions: 2748\n",
      "Accuracy: 0.4503 \n",
      "900 [D loss: 1.756713, acc.: 51.13%] [G loss: 0.162487] [Loss change: 0.162, Loss increases: 0]\n",
      "Number of right predictions: 2651\n",
      "Number of wrong predictions: 2348\n",
      "Accuracy: 0.5303 \n",
      "950 [D loss: 1.765468, acc.: 51.08%] [G loss: 0.161519] [Loss change: 0.162, Loss increases: 0]\n",
      "Number of right predictions: 2671\n",
      "Number of wrong predictions: 2328\n",
      "Accuracy: 0.5343 \n",
      "1000 [D loss: 1.721893, acc.: 51.24%] [G loss: 0.178701] [Loss change: 0.179, Loss increases: 0]\n",
      "Number of right predictions: 2777\n",
      "Number of wrong predictions: 2222\n",
      "Accuracy: 0.5555 \n",
      "1050 [D loss: 1.746842, acc.: 51.10%] [G loss: 0.176720] [Loss change: 0.177, Loss increases: 0]\n",
      "Number of right predictions: 2993\n",
      "Number of wrong predictions: 2006\n",
      "Accuracy: 0.5987 \n",
      "1100 [D loss: 1.715434, acc.: 51.02%] [G loss: 0.179786] [Loss change: 0.180, Loss increases: 0]\n",
      "Number of right predictions: 3072\n",
      "Number of wrong predictions: 1927\n",
      "Accuracy: 0.6145 \n",
      "1150 [D loss: 1.695443, acc.: 51.00%] [G loss: 0.180097] [Loss change: 0.180, Loss increases: 0]\n",
      "Number of right predictions: 3136\n",
      "Number of wrong predictions: 1863\n",
      "Accuracy: 0.6273 \n",
      "1200 [D loss: 1.713784, acc.: 51.00%] [G loss: 0.173160] [Loss change: 0.173, Loss increases: 0]\n",
      "Number of right predictions: 3431\n",
      "Number of wrong predictions: 1568\n",
      "Accuracy: 0.6863 \n",
      "1250 [D loss: 1.676551, acc.: 50.70%] [G loss: 0.173088] [Loss change: 0.173, Loss increases: 0]\n",
      "Number of right predictions: 4981\n",
      "Number of wrong predictions: 18\n",
      "Accuracy: 0.9964 \n",
      "1300 [D loss: 1.664351, acc.: 50.68%] [G loss: 0.178622] [Loss change: 0.179, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1350 [D loss: 1.661333, acc.: 50.34%] [G loss: 0.177535] [Loss change: 0.178, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1400 [D loss: 1.634317, acc.: 50.30%] [G loss: 0.193658] [Loss change: 0.194, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1450 [D loss: 1.619278, acc.: 50.78%] [G loss: 0.196361] [Loss change: 0.196, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1500 [D loss: 1.583946, acc.: 50.53%] [G loss: 0.201598] [Loss change: 0.202, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1550 [D loss: 1.571686, acc.: 50.89%] [G loss: 0.192902] [Loss change: 0.193, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1600 [D loss: 1.529718, acc.: 51.08%] [G loss: 0.192632] [Loss change: 0.193, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1650 [D loss: 1.510783, acc.: 51.02%] [G loss: 0.196406] [Loss change: 0.196, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1700 [D loss: 1.447062, acc.: 51.10%] [G loss: 0.200633] [Loss change: 0.201, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n",
      "1750 [D loss: 1.434654, acc.: 51.07%] [G loss: 0.199190] [Loss change: 0.199, Loss increases: 0]\n",
      "Number of right predictions: 4999\n",
      "Number of wrong predictions: 0\n",
      "Accuracy: 1.0000 \n"
     ]
    }
   ],
   "source": [
    "#training GAN model\n",
    "train_loop(combined, discriminator, generator, 3000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
